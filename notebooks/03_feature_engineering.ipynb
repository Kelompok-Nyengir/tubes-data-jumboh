{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03. Advanced Temporal Feature Engineering\n",
    "\n",
    "**Credit Card Default Analysis - Feature Engineering Phase**\n",
    "- **Repository**: Kelompok-Nyengir/tubes-data-jumboh\n",
    "- **Phase**: 3 of 5 - Advanced Feature Creation\n",
    "\n",
    "## üìã Notebook Objectives\n",
    "\n",
    "1. **Temporal Feature Engineering**: Create 25+ features from 6-month payment history\n",
    "2. **Payment Behavior Analysis**: Develop behavioral indicators and trends\n",
    "3. **Credit Utilization Features**: Engineer credit usage and efficiency metrics\n",
    "4. **Risk Scoring Components**: Build comprehensive risk assessment features\n",
    "5. **Customer Segmentation**: Create business-oriented customer categories\n",
    "\n",
    "## üéØ Expected Outcomes\n",
    "- Rich feature set for machine learning models\n",
    "- Temporal insights into payment behaviors\n",
    "- Business-interpretable risk indicators\n",
    "- Enhanced predictive capabilities"
   ],
   "id": "64da36b88ac5d7a3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ],
   "id": "ca6c66fe2165ffef"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced setup for feature engineering\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../src')\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Import custom modules\n",
    "from src.feature_engineering import TemporalFeatureEngineer\n",
    "from src.visualization import CreditCardVisualizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette('Set2')\n",
    "%matplotlib inline\n",
    "\n",
    "print('=' * 80)\n",
    "print('‚öôÔ∏è CREDIT CARD DEFAULT ANALYSIS - FEATURE ENGINEERING')\n",
    "print('=' * 80)\n",
    "print(f'üìÖ Analysis Date: 2025-06-20 16:04:47 UTC')\n",
    "print(f'üë§ Analyst: ardzz')\n",
    "print(f'üìù Phase: 3 of 5 - Advanced Feature Creation')\n",
    "print(f'üîó Repository: Kelompok-Nyengir/tubes-data-jumboh')\n",
    "print('=' * 80)"
   ],
   "id": "25c4e94df1245744"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('CreditCardFeatureEngineering') \\\n",
    "    .config('spark.sql.adaptive.enabled', 'true') \\\n",
    "    .config('spark.sql.adaptive.coalescePartitions.enabled', 'true') \\\n",
    "    .config('spark.serializer', 'org.apache.spark.serializer.KryoSerializer') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel('WARN')\n",
    "\n",
    "print(f'‚úÖ Spark Session initialized successfully')\n",
    "print(f'   Spark Version: {spark.version}')\n",
    "print(f'   Spark UI: {spark.sparkContext.uiWebUrl}')\n",
    "\n",
    "# Initialize feature engineer and visualizer\n",
    "feature_engineer = TemporalFeatureEngineer()\n",
    "visualizer = CreditCardVisualizer()\n",
    "\n",
    "print(f'‚úÖ Custom modules initialized')"
   ],
   "id": "cba9049b05330d8b"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Pre-Engineering Assessment"
   ],
   "id": "3e95f00c8abcc0bd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cleaned dataset from previous phase\n",
    "print('üìÇ Loading cleaned dataset for feature engineering...')\n",
    "\n",
    "try:\n",
    "    # Try to load cleaned data from previous phase\n",
    "    df_clean = spark.read.parquet('../data/processed/02_cleaned_data.parquet')\n",
    "    print(f'‚úÖ Loaded cleaned dataset from Phase 2')\n",
    "except:\n",
    "    try:\n",
    "        # Fallback to exploration cache\n",
    "        df_clean = spark.read.parquet('../data/processed/01_exploration_cache.parquet')\n",
    "        print(f'‚ö†Ô∏è  Using exploration cache - applying basic cleaning')\n",
    "        \n",
    "        # Apply basic cleaning\n",
    "        df_clean = df_clean.withColumn(\n",
    "            'EDUCATION', when(col('EDUCATION').isin([0, 5, 6]), 4).otherwise(col('EDUCATION'))\n",
    "        ).withColumn(\n",
    "            'MARRIAGE', when(col('MARRIAGE') == 0, 3).otherwise(col('MARRIAGE'))\n",
    "        )\n",
    "    except:\n",
    "        # Final fallback to original CSV with cleaning\n",
    "        from src.data_processing import CreditCardDataProcessor\n",
    "        processor = CreditCardDataProcessor(spark)\n",
    "        \n",
    "        try:\n",
    "            df_raw = processor.load_data('../data/sample.csv')\n",
    "        except:\n",
    "            df_raw = processor.load_data('../sample.csv')\n",
    "        \n",
    "        df_clean = processor.clean_data(df_raw)\n",
    "        print(f'‚ö†Ô∏è  Loaded and cleaned original CSV')\n",
    "\n",
    "# Dataset assessment\n",
    "print(f'\\nüìä PRE-FEATURE ENGINEERING ASSESSMENT:')\n",
    "print(f'   Records: {df_clean.count():,}')\n",
    "print(f'   Columns: {len(df_clean.columns)}')\n",
    "\n",
    "# Check for required columns\n",
    "required_columns = {\n",
    "    'demographic': ['LIMIT_BAL', 'SEX', 'EDUCATION', 'MARRIAGE', 'AGE'],\n",
    "    'payment_status': ['PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6'],\n",
    "    'bill_amounts': ['BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6'],\n",
    "    'payment_amounts': ['PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']\n",
    "}\n",
    "\n",
    "print(f'\\nüìã REQUIRED COLUMNS AVAILABILITY:')\n",
    "for category, cols in required_columns.items():\n",
    "    available = sum(1 for col in cols if col in df_clean.columns)\n",
    "    print(f'   {category.title()}: {available}/{len(cols)} columns available')\n",
    "    \n",
    "    missing = [col for col in cols if col not in df_clean.columns]\n",
    "    if missing:\n",
    "        print(f'      Missing: {', '.join(missing)}')\n",
    "\n",
    "print(f'\\n‚úÖ Dataset ready for feature engineering')"
   ],
   "id": "25dc98c45258c754"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1: Payment Trend and Volatility Features"
   ],
   "id": "fc6d2be80e2fdaf1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create payment trend and volatility features\n",
    "print('üìà PHASE 1: PAYMENT TREND AND VOLATILITY FEATURES')\n",
    "print('=' * 60)\n",
    "\n",
    "# Apply payment trend feature engineering\n",
    "df_features = feature_engineer.create_payment_trend_features(df_clean)\n",
    "\n",
    "# Analyze the created features\n",
    "trend_features = [\n",
    "    'PAYMENT_TREND_SLOPE', 'PAYMENT_STATUS_VOLATILITY', \n",
    "    'MAX_PAYMENT_DELAY', 'MIN_PAYMENT_DELAY', 'PAYMENT_DELAY_RANGE'\n",
    "]\n",
    "\n",
    "print(f'\\nüìä PAYMENT TREND FEATURES ANALYSIS:')\n",
    "print(f'{'Feature':<25} {'Mean':<10} {'Std':<10} {'Min':<10} {'Max':<10}')\n",
    "print('-' * 70)\n",
    "\n",
    "for feature in trend_features:\n",
    "    if feature in df_features.columns:\n",
    "        stats = df_features.select(\n",
    "            avg(feature).alias('mean'),\n",
    "            stddev(feature).alias('std'),\n",
    "            min(feature).alias('min'),\n",
    "            max(feature).alias('max')\n",
    "        ).collect()[0]\n",
    "        \n",
    "        print(f'{feature:<25} {stats['mean']:<10.3f} {stats['std']:<10.3f} '\n",
    "              f'{stats['min']:<10.3f} {stats['max']:<10.3f}')\n",
    "\n",
    "# Analyze payment trend slope distribution\n",
    "if 'PAYMENT_TREND_SLOPE' in df_features.columns:\n",
    "    print(f'\\nüîç PAYMENT TREND SLOPE INTERPRETATION:')\n",
    "    \n",
    "    # Categorize trend slopes\n",
    "    trend_categories = df_features.withColumn(\n",
    "        'TREND_CATEGORY',\n",
    "        when(col('PAYMENT_TREND_SLOPE') > 1, 'Strongly Improving')\n",
    "        .when(col('PAYMENT_TREND_SLOPE') > 0.2, 'Improving')\n",
    "        .when(col('PAYMENT_TREND_SLOPE') > -0.2, 'Stable')\n",
    "        .when(col('PAYMENT_TREND_SLOPE') > -1, 'Deteriorating')\n",
    "        .otherwise('Strongly Deteriorating')\n",
    "    ).groupBy('TREND_CATEGORY').count().orderBy(desc('count'))\n",
    "    \n",
    "    total_customers = df_features.count()\n",
    "    print(f'   Payment Trend Distribution:')\n",
    "    for row in trend_categories.collect():\n",
    "        category = row['TREND_CATEGORY']\n",
    "        count = row['count']\n",
    "        percentage = count / total_customers * 100\n",
    "        print(f'      {category}: {count:,} ({percentage:.1f}%)')\n",
    "\n",
    "# Payment volatility analysis\n",
    "if 'PAYMENT_STATUS_VOLATILITY' in df_features.columns:\n",
    "    print(f'\\nüìä PAYMENT VOLATILITY ANALYSIS:')\n",
    "    \n",
    "    volatility_percentiles = df_features.select(\n",
    "        expr('percentile_approx(PAYMENT_STATUS_VOLATILITY, 0.25)').alias('q1'),\n",
    "        expr('percentile_approx(PAYMENT_STATUS_VOLATILITY, 0.5)').alias('median'),\n",
    "        expr('percentile_approx(PAYMENT_STATUS_VOLATILITY, 0.75)').alias('q3'),\n",
    "        expr('percentile_approx(PAYMENT_STATUS_VOLATILITY, 0.9)').alias('p90')\n",
    "    ).collect()[0]\n",
    "    \n",
    "    print(f'   Volatility Percentiles:')\n",
    "    print(f'      25th percentile: {volatility_percentiles['q1']:.3f}')\n",
    "    print(f'      Median: {volatility_percentiles['median']:.3f}')\n",
    "    print(f'      75th percentile: {volatility_percentiles['q3']:.3f}')\n",
    "    print(f'      90th percentile: {volatility_percentiles['p90']:.3f}')\n",
    "    \n",
    "    # High volatility customers\n",
    "    high_volatility_threshold = volatility_percentiles['p90']\n",
    "    high_volatility_count = df_features.filter(\n",
    "        col('PAYMENT_STATUS_VOLATILITY') >= high_volatility_threshold\n",
    "    ).count()\n",
    "    \n",
    "    print(f'\\n   High Volatility Customers (>90th percentile):')\n",
    "    print(f'      Count: {high_volatility_count:,} ({high_volatility_count/total_customers*100:.1f}%)')\n",
    "    print(f'      Threshold: {high_volatility_threshold:.3f}')\n",
    "\n",
    "print(f'\\n‚úÖ Phase 1 completed: {len(trend_features)} trend and volatility features created')"
   ],
   "id": "65cd4ac35d9fe0dd"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2: Temporal Segmentation Features"
   ],
   "id": "5c4856bd7069ec1e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temporal segmentation features\n",
    "print('üïí PHASE 2: TEMPORAL SEGMENTATION FEATURES')\n",
    "print('=' * 60)\n",
    "\n",
    "# Apply temporal segmentation feature engineering\n",
    "df_features = feature_engineer.create_temporal_segmentation_features(df_features)\n",
    "\n",
    "# Analyze temporal segmentation features\n",
    "temporal_features = [\n",
    "    'RECENT_AVG_DELAY', 'HISTORICAL_AVG_DELAY', \n",
    "    'PAYMENT_IMPROVEMENT_SCORE', 'RECOVERY_INSTANCES'\n",
    "]\n",
    "\n",
    "print(f'\\nüìä TEMPORAL SEGMENTATION FEATURES ANALYSIS:')\n",
    "print(f'{'Feature':<25} {'Mean':<10} {'Std':<10} {'Min':<10} {'Max':<10}')\n",
    "print('-' * 70)\n",
    "\n",
    "for feature in temporal_features:\n",
    "    if feature in df_features.columns:\n",
    "        stats = df_features.select(\n",
    "            avg(feature).alias('mean'),\n",
    "            stddev(feature).alias('std'),\n",
    "            min(feature).alias('min'),\n",
    "            max(feature).alias('max')\n",
    "        ).collect()[0]\n",
    "        \n",
    "        print(f'{feature:<25} {stats['mean']:<10.3f} {stats['std']:<10.3f} '\n",
    "              f'{stats['min']:<10.3f} {stats['max']:<10.3f}')\n",
    "\n",
    "# Payment improvement analysis\n",
    "if 'PAYMENT_IMPROVEMENT_SCORE' in df_features.columns:\n",
    "    print(f'\\nüìà PAYMENT IMPROVEMENT ANALYSIS:')\n",
    "    \n",
    "    # Categorize improvement scores\n",
    "    improvement_categories = df_features.withColumn(\n",
    "        'IMPROVEMENT_CATEGORY',\n",
    "        when(col('PAYMENT_IMPROVEMENT_SCORE') > 2, 'Significant Improvement')\n",
    "        .when(col('PAYMENT_IMPROVEMENT_SCORE') > 0.5, 'Moderate Improvement')\n",
    "        .when(col('PAYMENT_IMPROVEMENT_SCORE') > -0.5, 'Stable')\n",
    "        .when(col('PAYMENT_IMPROVEMENT_SCORE') > -2, 'Moderate Deterioration')\n",
    "        .otherwise('Significant Deterioration')\n",
    "    ).groupBy('IMPROVEMENT_CATEGORY').count().orderBy(desc('count'))\n",
    "    \n",
    "    print(f'   Payment Improvement Distribution:')\n",
    "    for row in improvement_categories.collect():\n",
    "        category = row['IMPROVEMENT_CATEGORY']\n",
    "        count = row['count']\n",
    "        percentage = count / df_features.count() * 100\n",
    "        print(f'      {category}: {count:,} ({percentage:.1f}%)')\n",
    "    \n",
    "    # Correlation with default\n",
    "    if 'default payment next month' in df_features.columns:\n",
    "        # Calculate correlation\n",
    "        assembler = VectorAssembler(\n",
    "            inputCols=['PAYMENT_IMPROVEMENT_SCORE', 'default payment next month'], \n",
    "            outputCol='features'\n",
    "        )\n",
    "        df_corr = assembler.transform(df_features).select('features')\n",
    "        correlation_matrix = Correlation.corr(df_corr, 'features').head()[0]\n",
    "        correlation_value = float(correlation_matrix.toArray()[0, 1])\n",
    "        \n",
    "        print(f'\\n   Correlation with Default: {correlation_value:.4f}')\n",
    "        if abs(correlation_value) > 0.1:\n",
    "            direction = 'negative' if correlation_value < 0 else 'positive'\n",
    "            strength = 'strong' if abs(correlation_value) > 0.3 else 'moderate' if abs(correlation_value) > 0.1 else 'weak'\n",
    "            print(f'      {strength.title()} {direction} correlation detected')\n",
    "\n",
    "# Recovery instances analysis\n",
    "if 'RECOVERY_INSTANCES' in df_features.columns:\n",
    "    print(f'\\nüîÑ RECOVERY INSTANCES ANALYSIS:')\n",
    "    \n",
    "    recovery_distribution = df_features.groupBy('RECOVERY_INSTANCES').count().orderBy('RECOVERY_INSTANCES')\n",
    "    \n",
    "    print(f'   Recovery Instances Distribution:')\n",
    "    print(f'   {'Instances':<12} {'Count':<10} {'Percentage':<12} {'Interpretation'}')\n",
    "    print('-' * 55)\n",
    "    \n",
    "    for row in recovery_distribution.collect():\n",
    "        instances = row['RECOVERY_INSTANCES']\n",
    "        count = row['count']\n",
    "        percentage = count / df_features.count() * 100\n",
    "        \n",
    "        if instances == 0:\n",
    "            interpretation = 'No recovery'\n",
    "        elif instances <= 2:\n",
    "            interpretation = 'Limited recovery'\n",
    "        elif instances <= 4:\n",
    "            interpretation = 'Good recovery'\n",
    "        else:\n",
    "            interpretation = 'Excellent recovery'\n",
    "        \n",
    "        print(f'   {instances:<12} {count:<10,} {percentage:<12.1f}% {interpretation}')\n",
    "    \n",
    "    # Average recovery by default status\n",
    "    if 'default payment next month' in df_features.columns:\n",
    "        avg_recovery_by_default = df_features.groupBy('default payment next month') \\\n",
    "            .agg(avg('RECOVERY_INSTANCES').alias('avg_recovery')) \\\n",
    "            .orderBy('default payment next month')\n",
    "        \n",
    "        print(f'\\n   Average Recovery by Default Status:')\n",
    "        for row in avg_recovery_by_default.collect():\n",
    "            default_status = 'No Default' if row['default payment next month'] == 0 else 'Default'\n",
    "            avg_recovery = row['avg_recovery']\n",
    "            print(f'      {default_status}: {avg_recovery:.2f} recovery instances')\n",
    "\n",
    "print(f'\\n‚úÖ Phase 2 completed: {len(temporal_features)} temporal segmentation features created')"
   ],
   "id": "46a70e423cd42a6e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 3: Bill Statement and Financial Features"
   ],
   "id": "7a86aa41e8aea5d7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bill statement and financial features\n",
    "print('üí∞ PHASE 3: BILL STATEMENT AND FINANCIAL FEATURES')\n",
    "print('=' * 60)\n",
    "\n",
    "# Apply bill statement feature engineering\n",
    "df_features = feature_engineer.create_bill_statement_features(df_features)\n",
    "\n",
    "# Analyze bill statement features\n",
    "bill_features = [\n",
    "    'BILL_TREND_SLOPE', 'BILL_AMOUNT_VOLATILITY', \n",
    "    'DEBT_ACCUMULATION_RATE', 'AVG_BILL_AMOUNT'\n",
    "]\n",
    "\n",
    "print(f'\\nüìä BILL STATEMENT FEATURES ANALYSIS:')\n",
    "print(f'{'Feature':<25} {'Mean':<12} {'Std':<12} {'Min':<12} {'Max':<12}')\n",
    "print('-' * 80)\n",
    "\n",
    "for feature in bill_features:\n",
    "    if feature in df_features.columns:\n",
    "        stats = df_features.select(\n",
    "            avg(feature).alias('mean'),\n",
    "            stddev(feature).alias('std'),\n",
    "            min(feature).alias('min'),\n",
    "            max(feature).alias('max')\n",
    "        ).collect()[0]\n",
    "        \n",
    "        print(f'{feature:<25} {stats['mean']:<12.2f} {stats['std']:<12.2f} '\n",
    "              f'{stats['min']:<12.2f} {stats['max']:<12.2f}')\n",
    "\n",
    "# Bill trend analysis\n",
    "if 'BILL_TREND_SLOPE' in df_features.columns:\n",
    "    print(f'\\nüìà BILL TREND ANALYSIS:')\n",
    "    \n",
    "    # Categorize bill trends\n",
    "    bill_trend_categories = df_features.withColumn(\n",
    "        'BILL_TREND_CATEGORY',\n",
    "        when(col('BILL_TREND_SLOPE') > 5000, 'Rapidly Increasing')\n",
    "        .when(col('BILL_TREND_SLOPE') > 1000, 'Increasing')\n",
    "        .when(col('BILL_TREND_SLOPE') > -1000, 'Stable')\n",
    "        .when(col('BILL_TREND_SLOPE') > -5000, 'Decreasing')\n",
    "        .otherwise('Rapidly Decreasing')\n",
    "    ).groupBy('BILL_TREND_CATEGORY').count().orderBy(desc('count'))\n",
    "    \n",
    "    print(f'   Bill Trend Distribution:')\n",
    "    for row in bill_trend_categories.collect():\n",
    "        category = row['BILL_TREND_CATEGORY']\n",
    "        count = row['count']\n",
    "        percentage = count / df_features.count() * 100\n",
    "        print(f'      {category}: {count:,} ({percentage:.1f}%)')\n",
    "\n",
    "# Debt accumulation analysis\n",
    "if 'DEBT_ACCUMULATION_RATE' in df_features.columns:\n",
    "    print(f'\\nüìä DEBT ACCUMULATION ANALYSIS:')\n",
    "    \n",
    "    # Analyze debt accumulation patterns\n",
    "    debt_percentiles = df_features.select(\n",
    "        expr('percentile_approx(DEBT_ACCUMULATION_RATE, 0.1)').alias('p10'),\n",
    "        expr('percentile_approx(DEBT_ACCUMULATION_RATE, 0.25)').alias('q1'),\n",
    "        expr('percentile_approx(DEBT_ACCUMULATION_RATE, 0.5)').alias('median'),\n",
    "        expr('percentile_approx(DEBT_ACCUMULATION_RATE, 0.75)').alias('q3'),\n",
    "        expr('percentile_approx(DEBT_ACCUMULATION_RATE, 0.9)').alias('p90')\n",
    "    ).collect()[0]\n",
    "    \n",
    "    print(f'   Debt Accumulation Rate Percentiles:')\n",
    "    print(f'      10th percentile: {debt_percentiles['p10']:.3f}')\n",
    "    print(f'      25th percentile: {debt_percentiles['q1']:.3f}')\n",
    "    print(f'      Median: {debt_percentiles['median']:.3f}')\n",
    "    print(f'      75th percentile: {debt_percentiles['q3']:.3f}')\n",
    "    print(f'      90th percentile: {debt_percentiles['p90']:.3f}')\n",
    "    \n",
    "    # High debt accumulation customers\n",
    "    high_debt_threshold = 0.5  # 50% increase\n",
    "    high_debt_count = df_features.filter(col('DEBT_ACCUMULATION_RATE') > high_debt_threshold).count()\n",
    "    \n",
    "    print(f'\\n   High Debt Accumulation (>50% increase):')\n",
    "    print(f'      Count: {high_debt_count:,} ({high_debt_count/df_features.count()*100:.1f}%)')\n",
    "    \n",
    "    # Negative debt accumulation (debt reduction)\n",
    "    debt_reduction_count = df_features.filter(col('DEBT_ACCUMULATION_RATE') < -0.1).count()\n",
    "    print(f'\\n   Debt Reduction (>10% decrease):')\n",
    "    print(f'      Count: {debt_reduction_count:,} ({debt_reduction_count/df_features.count()*100:.1f}%)')\n",
    "\n",
    "# Average bill amount analysis\n",
    "if 'AVG_BILL_AMOUNT' in df_features.columns and 'LIMIT_BAL' in df_features.columns:\n",
    "    print(f'\\nüí≥ CREDIT UTILIZATION ANALYSIS:')\n",
    "    \n",
    "    # Calculate basic credit utilization\n",
    "    utilization_stats = df_features.withColumn(\n",
    "        'BASIC_UTILIZATION',\n",
    "        when(col('LIMIT_BAL') > 0, col('AVG_BILL_AMOUNT') / col('LIMIT_BAL')).otherwise(0)\n",
    "    ).select(\n",
    "        avg('BASIC_UTILIZATION').alias('avg_util'),\n",
    "        expr('percentile_approx(BASIC_UTILIZATION, 0.5)').alias('median_util'),\n",
    "        expr('percentile_approx(BASIC_UTILIZATION, 0.9)').alias('p90_util')\n",
    "    ).collect()[0]\n",
    "    \n",
    "    print(f'   Credit Utilization Statistics:')\n",
    "    print(f'      Average utilization: {utilization_stats['avg_util']:.3f} ({utilization_stats['avg_util']*100:.1f}%)')\n",
    "    print(f'      Median utilization: {utilization_stats['median_util']:.3f} ({utilization_stats['median_util']*100:.1f}%)')\n",
    "    print(f'      90th percentile: {utilization_stats['p90_util']:.3f} ({utilization_stats['p90_util']*100:.1f}%)')\n",
    "    \n",
    "    # High utilization customers\n",
    "    high_util_count = df_features.filter(\n",
    "        (col('LIMIT_BAL') > 0) & (col('AVG_BILL_AMOUNT') / col('LIMIT_BAL') > 0.8)\n",
    "    ).count()\n",
    "    \n",
    "    print(f'\\n   High Utilization (>80%):')\n",
    "    print(f'      Count: {high_util_count:,} ({high_util_count/df_features.count()*100:.1f}%)')\n",
    "\n",
    "print(f'\\n‚úÖ Phase 3 completed: {len(bill_features)} bill statement and financial features created')"
   ],
   "id": "871f56f4ceceba9e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 4: Payment Efficiency and Consistency Features"
   ],
   "id": "7a8baec2610e2079"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create payment efficiency and consistency features\n",
    "print('‚ö° PHASE 4: PAYMENT EFFICIENCY AND CONSISTENCY FEATURES')\n",
    "print('=' * 60)\n",
    "\n",
    "# Apply payment efficiency feature engineering\n",
    "df_features = feature_engineer.create_payment_efficiency_features(df_features)\n",
    "\n",
    "# Analyze payment efficiency features\n",
    "efficiency_features = [\n",
    "    'AVG_PAYMENT_EFFICIENCY', 'PAYMENT_EFFICIENCY_TREND',\n",
    "    'PAYMENT_CONSISTENCY_SCORE', 'AVG_PAYMENT_AMOUNT'\n",
    "]\n",
    "\n",
    "print(f'\\nüìä PAYMENT EFFICIENCY FEATURES ANALYSIS:')\n",
    "print(f'{'Feature':<25} {'Mean':<12} {'Std':<12} {'Min':<12} {'Max':<12}')\n",
    "print('-' * 80)\n",
    "\n",
    "for feature in efficiency_features:\n",
    "    if feature in df_features.columns:\n",
    "        stats = df_features.select(\n",
    "            avg(feature).alias('mean'),\n",
    "            stddev(feature).alias('std'),\n",
    "            min(feature).alias('min'),\n",
    "            max(feature).alias('max')\n",
    "        ).collect()[0]\n",
    "        \n",
    "        print(f'{feature:<25} {stats['mean']:<12.3f} {stats['std']:<12.3f} '\n",
    "              f'{stats['min']:<12.3f} {stats['max']:<12.3f}')\n",
    "\n",
    "# Payment efficiency analysis\n",
    "if 'AVG_PAYMENT_EFFICIENCY' in df_features.columns:\n",
    "    print(f'\\n‚ö° PAYMENT EFFICIENCY ANALYSIS:')\n",
    "    \n",
    "    # Categorize payment efficiency\n",
    "    efficiency_categories = df_features.withColumn(\n",
    "        'EFFICIENCY_CATEGORY',\n",
    "        when(col('AVG_PAYMENT_EFFICIENCY') >= 1.0, 'Full Payment')\n",
    "        .when(col('AVG_PAYMENT_EFFICIENCY') >= 0.8, 'High Efficiency')\n",
    "        .when(col('AVG_PAYMENT_EFFICIENCY') >= 0.5, 'Medium Efficiency')\n",
    "        .when(col('AVG_PAYMENT_EFFICIENCY') >= 0.2, 'Low Efficiency')\n",
    "        .otherwise('Very Low Efficiency')\n",
    "    ).groupBy('EFFICIENCY_CATEGORY').count().orderBy(desc('count'))\n",
    "    \n",
    "    print(f'   Payment Efficiency Distribution:')\n",
    "    for row in efficiency_categories.collect():\n",
    "        category = row['EFFICIENCY_CATEGORY']\n",
    "        count = row['count']\n",
    "        percentage = count / df_features.count() * 100\n",
    "        print(f'      {category}: {count:,} ({percentage:.1f}%)')\n",
    "    \n",
    "    # Efficiency vs default correlation\n",
    "    if 'default payment next month' in df_features.columns:\n",
    "        efficiency_by_default = df_features.groupBy('default payment next month') \\\n",
    "            .agg(avg('AVG_PAYMENT_EFFICIENCY').alias('avg_efficiency')) \\\n",
    "            .orderBy('default payment next month')\n",
    "        \n",
    "        print(f'\\n   Average Efficiency by Default Status:')\n",
    "        for row in efficiency_by_default.collect():\n",
    "            default_status = 'No Default' if row['default payment next month'] == 0 else 'Default'\n",
    "            avg_efficiency = row['avg_efficiency']\n",
    "            print(f'      {default_status}: {avg_efficiency:.3f} ({avg_efficiency*100:.1f}%)')\n",
    "\n",
    "# Payment consistency analysis\n",
    "if 'PAYMENT_CONSISTENCY_SCORE' in df_features.columns:\n",
    "    print(f'\\nüìä PAYMENT CONSISTENCY ANALYSIS:')\n",
    "    \n",
    "    # Analyze consistency score distribution\n",
    "    consistency_percentiles = df_features.select(\n",
    "        expr('percentile_approx(PAYMENT_CONSISTENCY_SCORE, 0.1)').alias('p10'),\n",
    "        expr('percentile_approx(PAYMENT_CONSISTENCY_SCORE, 0.25)').alias('q1'),\n",
    "        expr('percentile_approx(PAYMENT_CONSISTENCY_SCORE, 0.5)').alias('median'),\n",
    "        expr('percentile_approx(PAYMENT_CONSISTENCY_SCORE, 0.75)').alias('q3'),\n",
    "        expr('percentile_approx(PAYMENT_CONSISTENCY_SCORE, 0.9)').alias('p90')\n",
    "    ).collect()[0]\n",
    "    \n",
    "    print(f'   Consistency Score Percentiles:')\n",
    "    print(f'      10th percentile: {consistency_percentiles['p10']:.3f}')\n",
    "    print(f'      25th percentile: {consistency_percentiles['q1']:.3f}')\n",
    "    print(f'      Median: {consistency_percentiles['median']:.3f}')\n",
    "    print(f'      75th percentile: {consistency_percentiles['q3']:.3f}')\n",
    "    print(f'      90th percentile: {consistency_percentiles['p90']:.3f}')\n",
    "    \n",
    "    # High consistency customers\n",
    "    high_consistency_threshold = consistency_percentiles['q3']\n",
    "    high_consistency_count = df_features.filter(\n",
    "        col('PAYMENT_CONSISTENCY_SCORE') >= high_consistency_threshold\n",
    "    ).count()\n",
    "    \n",
    "    print(f'\\n   High Consistency (‚â•75th percentile):')\n",
    "    print(f'      Count: {high_consistency_count:,} ({high_consistency_count/df_features.count()*100:.1f}%)')\n",
    "    print(f'      Threshold: {high_consistency_threshold:.3f}')\n",
    "\n",
    "# Payment efficiency trend analysis\n",
    "if 'PAYMENT_EFFICIENCY_TREND' in df_features.columns:\n",
    "    print(f'\\nüìà PAYMENT EFFICIENCY TREND ANALYSIS:')\n",
    "    \n",
    "    # Categorize efficiency trends\n",
    "    trend_categories = df_features.withColumn(\n",
    "        'EFFICIENCY_TREND_CATEGORY',\n",
    "        when(col('PAYMENT_EFFICIENCY_TREND') > 0.1, 'Improving Efficiency')\n",
    "        .when(col('PAYMENT_EFFICIENCY_TREND') > -0.1, 'Stable Efficiency')\n",
    "        .otherwise('Declining Efficiency')\n",
    "    ).groupBy('EFFICIENCY_TREND_CATEGORY').count().orderBy(desc('count'))\n",
    "    \n",
    "    print(f'   Efficiency Trend Distribution:')\n",
    "    for row in trend_categories.collect():\n",
    "        category = row['EFFICIENCY_TREND_CATEGORY']\n",
    "        count = row['count']\n",
    "        percentage = count / df_features.count() * 100\n",
    "        print(f'      {category}: {count:,} ({percentage:.1f}%)')\n",
    "\n",
    "print(f'\\n‚úÖ Phase 4 completed: {len(efficiency_features)} payment efficiency and consistency features created')"
   ],
   "id": "b59d6fe447b70300"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 5: Credit Utilization and Risk Features"
   ],
   "id": "bc6633468755a282"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create credit utilization features\n",
    "print('üí≥ PHASE 5: CREDIT UTILIZATION AND RISK FEATURES')\n",
    "print('=' * 60)\n",
    "\n",
    "# Apply credit utilization feature engineering\n",
    "df_features = feature_engineer.create_credit_utilization_features(df_features)\n",
    "\n",
    "# Analyze credit utilization features\n",
    "utilization_features = [\n",
    "    'CREDIT_UTILIZATION_RATIO', 'CREDIT_BUFFER', 'CREDIT_UTILIZATION_TREND'\n",
    "]\n",
    "\n",
    "print(f'\\nüìä CREDIT UTILIZATION FEATURES ANALYSIS:')\n",
    "print(f'{'Feature':<25} {'Mean':<12} {'Std':<12} {'Min':<12} {'Max':<12}')\n",
    "print('-' * 80)\n",
    "\n",
    "for feature in utilization_features:\n",
    "    if feature in df_features.columns:\n",
    "        stats = df_features.select(\n",
    "            avg(feature).alias('mean'),\n",
    "            stddev(feature).alias('std'),\n",
    "            min(feature).alias('min'),\n",
    "            max(feature).alias('max')\n",
    "        ).collect()[0]\n",
    "        \n",
    "        if feature == 'CREDIT_BUFFER':\n",
    "            # Display credit buffer in thousands\n",
    "            print(f'{feature:<25} {stats['mean']/1000:<12.1f}K {stats['std']/1000:<12.1f}K '\n",
    "                  f'{stats['min']/1000:<12.1f}K {stats['max']/1000:<12.1f}K')\n",
    "        else:\n",
    "            print(f'{feature:<25} {stats['mean']:<12.3f} {stats['std']:<12.3f} '\n",
    "                  f'{stats['min']:<12.3f} {stats['max']:<12.3f}')\n",
    "\n",
    "# Credit utilization ratio analysis\n",
    "if 'CREDIT_UTILIZATION_RATIO' in df_features.columns:\n",
    "    print(f'\\nüí≥ CREDIT UTILIZATION RATIO ANALYSIS:')\n",
    "    \n",
    "    # Categorize utilization ratios\n",
    "    utilization_categories = df_features.withColumn(\n",
    "        'UTILIZATION_CATEGORY',\n",
    "        when(col('CREDIT_UTILIZATION_RATIO') <= 0.1, 'Very Low (‚â§10%)')\n",
    "        .when(col('CREDIT_UTILIZATION_RATIO') <= 0.3, 'Low (10-30%)')\n",
    "        .when(col('CREDIT_UTILIZATION_RATIO') <= 0.5, 'Medium (30-50%)')\n",
    "        .when(col('CREDIT_UTILIZATION_RATIO') <= 0.8, 'High (50-80%)')\n",
    "        .when(col('CREDIT_UTILIZATION_RATIO') <= 1.0, 'Very High (80-100%)')\n",
    "        .otherwise('Over Limit (>100%)')\n",
    "    ).groupBy('UTILIZATION_CATEGORY').count().orderBy(desc('count'))\n",
    "    \n",
    "    print(f'   Credit Utilization Distribution:')\n",
    "    for row in utilization_categories.collect():\n",
    "        category = row['UTILIZATION_CATEGORY']\n",
    "        count = row['count']\n",
    "        percentage = count / df_features.count() * 100\n",
    "        print(f'      {category}: {count:,} ({percentage:.1f}%)')\n",
    "    \n",
    "    # Utilization vs default analysis\n",
    "    if 'default payment next month' in df_features.columns:\n",
    "        utilization_default = df_features.groupBy('default payment next month') \\\n",
    "            .agg(avg('CREDIT_UTILIZATION_RATIO').alias('avg_utilization')) \\\n",
    "            .orderBy('default payment next month')\n",
    "        \n",
    "        print(f'\\n   Average Utilization by Default Status:')\n",
    "        for row in utilization_default.collect():\n",
    "            default_status = 'No Default' if row['default payment next month'] == 0 else 'Default'\n",
    "            avg_utilization = row['avg_utilization']\n",
    "            print(f'      {default_status}: {avg_utilization:.3f} ({avg_utilization*100:.1f}%)')\n",
    "\n",
    "# Credit buffer analysis\n",
    "if 'CREDIT_BUFFER' in df_features.columns:\n",
    "    print(f'\\nüí∞ CREDIT BUFFER ANALYSIS:')\n",
    "    \n",
    "    # Analyze credit buffer distribution\n",
    "    buffer_stats = df_features.select(\n",
    "        avg('CREDIT_BUFFER').alias('avg_buffer'),\n",
    "        expr('percentile_approx(CREDIT_BUFFER, 0.25)').alias('q1'),\n",
    "        expr('percentile_approx(CREDIT_BUFFER, 0.5)').alias('median'),\n",
    "        expr('percentile_approx(CREDIT_BUFFER, 0.75)').alias('q3')\n",
    "    ).collect()[0]\n",
    "    \n",
    "    print(f'   Credit Buffer Statistics (NT$):')\n",
    "    print(f'      Average: NT$ {buffer_stats['avg_buffer']:,.0f}')\n",
    "    print(f'      25th percentile: NT$ {buffer_stats['q1']:,.0f}')\n",
    "    print(f'      Median: NT$ {buffer_stats['median']:,.0f}')\n",
    "    print(f'      75th percentile: NT$ {buffer_stats['q3']:,.0f}')\n",
    "    \n",
    "    # Low buffer customers (potential risk)\n",
    "    low_buffer_count = df_features.filter(col('CREDIT_BUFFER') < 10000).count()\n",
    "    print(f'\\n   Low Buffer (<NT$ 10,000):')\n",
    "    print(f'      Count: {low_buffer_count:,} ({low_buffer_count/df_features.count()*100:.1f}%)')\n",
    "\n",
    "# Credit utilization trend analysis\n",
    "if 'CREDIT_UTILIZATION_TREND' in df_features.columns:\n",
    "    print(f'\\nüìà CREDIT UTILIZATION TREND ANALYSIS:')\n",
    "    \n",
    "    # Categorize utilization trends\n",
    "    trend_categories = df_features.withColumn(\n",
    "        'UTILIZATION_TREND_CATEGORY',\n",
    "        when(col('CREDIT_UTILIZATION_TREND') > 0.1, 'Increasing Utilization')\n",
    "        .when(col('CREDIT_UTILIZATION_TREND') > -0.1, 'Stable Utilization')\n",
    "        .otherwise('Decreasing Utilization')\n",
    "    ).groupBy('UTILIZATION_TREND_CATEGORY').count().orderBy(desc('count'))\n",
    "    \n",
    "    print(f'   Utilization Trend Distribution:')\n",
    "    for row in trend_categories.collect():\n",
    "        category = row['UTILIZATION_TREND_CATEGORY']\n",
    "        count = row['count']\n",
    "        percentage = count / df_features.count() * 100\n",
    "        print(f'      {category}: {count:,} ({percentage:.1f}%)')\n",
    "\n",
    "print(f'\\n‚úÖ Phase 5 completed: {len(utilization_features)} credit utilization features created')"
   ],
   "id": "17e8d45fdc701310"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 6: Behavioral Classification and Risk Scoring"
   ],
   "id": "fe9757701618531f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print('üéØ PHASE 6: BEHAVIORAL CLASSIFICATION AND RISK SCORING')\n",
    "print('=' * 60)\n",
    "    # Apply beavioral classification feature engineering\n",
    "df_features = src.feature_engineer.create_behavioral_classification_features(df_features)\n",
    "df_features = src.feature_engineer.create_risk_scoring_features(df_features)\n",
    "    \n",
    "behavioral_features = [\n",
    "    'PAYMENT_BEHAVIOR_TYPE', 'TEMPORAL_RISK_LEVEL', 'CUSTOMER_SEGMENT',\n",
    "    'TEMPORAL_RISK_SCORE', 'RISK_SCORE_CATEGORY'\n",
    "]\n",
    "    \n",
    "print(f'\\\\nüéØ BEHAVIORAL CLASSIFICATION ANALYSIS:')\n",
    "    \n",
    "if 'PAYMENT_BEHAVIOR_TYPE' in df_features.columns:\n",
    "    print(f'\\\\n   Payment Behavior Type Distribution:')\n",
    "    behavior_dist = df_features.groupBy('PAYMENT_BEHAVIOR_TYPE').count().orderBy(desc('count'))\n",
    "        \n",
    "    for row in behavior_dist.collect():\n",
    "        behavior_type = row['PAYMENT_BEHAVIOR_TYPE']\n",
    "        count = row['count']\n",
    "        percentage = count / df_features.count() * 100\n",
    "        print(f'      {behavior_type}: {count:,} ({percentage:.1f}%)')\n",
    "        \n",
    "    if 'default payment next month' in df_features.columns:\n",
    "        behavior_default = df_features.groupBy('PAYMENT_BEHAVIOR_TYPE', 'default payment next month') \\\\\n",
    "            .count()\n",
    "            .groupBy('PAYMENT_BEHAVIOR_TYPE')\n",
    "            .pivot('default payment next month')\n",
    "            .sum('count')\n",
    "            .fillna(0)\n",
    "            \n",
    "            behavior_default = behavior_default.withColumn(\n",
    "                'total', col('0') + col('1')\n",
    "            ).withColumn(\n",
    "                'default_rate', col('1') / col('total') * 100\n",
    "            )\n",
    "            \n",
    "            print(f'\\\\n   Default Rate by Payment Behavior Type:')\n",
    "            print(f'   {'Behavior Type':<20} {'No Default':<12} {'Default':<10} {'Default Rate':<12}')\n",
    "            print('   ' + '-' * 60)\n",
    "            \n",
    "            for row in behavior_default.collect():\n",
    "                behavior_type = row['PAYMENT_BEHAVIOR_TYPE']\n",
    "                no_default = int(row['0']) if row['0'] else 0\n",
    "                default = int(row['1']) if row['1'] else 0\n",
    "                default_rate = row['default_rate']\n",
    "                print(f'   {behavior_type:<20} {no_default:<12,} {default:<10,} {default_rate:<12.1f}%')\n",
    "    \n",
    "if 'TEMPORAL_RISK_SCORE' in df_features.columns:\n",
    "    print(f'\\\\nüìä TEMPORAL RISK SCORE ANALYSIS:')\n",
    "    \n",
    "    # Risk score statistics\n",
    "    risk_score_stats = df_features.select(\n",
    "        avg('TEMPORAL_RISK_SCORE').alias('avg_score'),\n",
    "        stddev('TEMPORAL_RISK_SCORE').alias('std_score'),\n",
    "        expr('percentile_approx(TEMPORAL_RISK_SCORE, 0.1)').alias('p10'),\n",
    "        expr('percentile_approx(TEMPORAL_RISK_SCORE, 0.25)').alias('q1'),\n",
    "        expr('percentile_approx(TEMPORAL_RISK_SCORE, 0.5)').alias('median'),\n",
    "        expr('percentile_approx(TEMPORAL_RISK_SCORE, 0.75)').alias('q3'),\n",
    "        expr('percentile_approx(TEMPORAL_RISK_SCORE, 0.9)').alias('p90')\n",
    "    ).collect()[0]\n",
    "    \n",
    "    print(f'   Risk Score Statistics:')\n",
    "    print(f'      Average: {risk_score_stats['avg_score']:.4f}')\n",
    "    print(f'      Std Dev: {risk_score_stats['std_score']:.4f}')\n",
    "    print(f'      10th percentile: {risk_score_stats['p10']:.4f}')\n",
    "    print(f'      25th percentile: {risk_score_stats['q1']:.4f}')\n",
    "    print(f'      Median: {risk_score_stats['median']:.4f}')\n",
    "    print(f'      75th percentile: {risk_score_stats['q3']:.4f}')\n",
    "    print(f'      90th percentile: {risk_score_stats['p90']:.4f}')\n",
    "\n",
    "# Risk score category analysis\n",
    "if 'RISK_SCORE_CATEGORY' in df_features.columns:\n",
    "    print(f'\\\\nüéØ RISK SCORE CATEGORY ANALYSIS:')\n",
    "    \n",
    "    risk_category_dist = df_features.groupBy('RISK_SCORE_CATEGORY').count().orderBy(desc('count'))\n",
    "    \n",
    "    print(f'   Risk Score Category Distribution:')\n",
    "    for row in risk_category_dist.collect():\n",
    "        category = row['RISK_SCORE_CATEGORY']\n",
    "        count = row['count']\n",
    "        percentage = count / df_features.count() * 100\n",
    "        print(f'      {category}: {count:,} ({percentage:.1f}%)')\n",
    "    \n",
    "    # Risk category vs default analysis\n",
    "    if 'default payment next month' in df_features.columns:\n",
    "        risk_default = df_features.groupBy('RISK_SCORE_CATEGORY', 'default payment next month') \\\n",
    "            .count() \\\n",
    "            .groupBy('RISK_SCORE_CATEGORY') \\\n",
    "            .pivot('default payment next month') \\\n",
    "            .sum('count') \\\n",
    "            .fillna(0)\n",
    "        \n",
    "        risk_default = risk_default.withColumn(\n",
    "            'total', col('0') + col('1')\n",
    "        ).withColumn(\n",
    "            'default_rate', col('1') / col('total') * 100\n",
    "        )\n",
    "        \n",
    "        print(f'\\\\n   Default Rate by Risk Category:')\n",
    "        print(f'   {'Risk Category':<15} {'No Default':<12} {'Default':<10} {'Default Rate':<12}')\n",
    "        print('   ' + '-' * 55)\n",
    "        \n",
    "        for row in risk_default.collect():\n",
    "            category = row['RISK_SCORE_CATEGORY']\n",
    "            no_default = int(row['0']) if row['0'] else 0\n",
    "            default = int(row['1']) if row['1'] else 0\n",
    "            default_rate = row['default_rate']\n",
    "            print(f'   {category:<15} {no_default:<12,} {default:<10,} {default_rate:<12.1f}%')\n",
    "\n",
    "print(f'\\\\n‚úÖ Phase 6 completed: Behavioral classification and risk scoring features created')\n",
    "# Comprehensive feature engineering summary\n",
    "print('üìã FEATURE ENGINEERING SUMMARY AND VALIDATION')\n",
    "print('=' * 60)\n",
    "\n",
    "# Count all created features\n",
    "original_columns = set(['ID', 'LIMIT_BAL', 'SEX', 'EDUCATION', 'MARRIAGE', 'AGE'] + \n",
    "                       [f'PAY_{i}' for i in [0, 2, 3, 4, 5, 6]] +\n",
    "                       [f'BILL_AMT{i}' for i in range(1, 7)] +\n",
    "                       [f'PAY_AMT{i}' for i in range(1, 7)] +\n",
    "                       ['default payment next month'])\n",
    "\n",
    "all_columns = set(df_features.columns)\n",
    "new_features = all_columns - original_columns\n",
    "\n",
    "print(f'\\\\nüìä FEATURE CREATION SUMMARY:')\n",
    "print(f'   Original columns: {len(original_columns)}')\n",
    "print(f'   Total columns after engineering: {len(all_columns)}')\n",
    "print(f'   New features created: {len(new_features)}')\n",
    "print(f'   Feature expansion: +{len(new_features)/len(original_columns)*100:.1f}%')\n",
    "\n",
    "# Categorize new features\n",
    "feature_categories = {\n",
    "    'Payment Trends': [f for f in new_features if 'TREND' in f or 'VOLATILITY' in f or 'DELAY' in f],\n",
    "    'Temporal Segmentation': [f for f in new_features if 'RECENT' in f or 'HISTORICAL' in f or 'IMPROVEMENT' in f or 'RECOVERY' in f],\n",
    "    'Financial Analysis': [f for f in new_features if 'BILL' in f or 'DEBT' in f or 'AVG_BILL' in f],\n",
    "    'Payment Efficiency': [f for f in new_features if 'EFFICIENCY' in f or 'CONSISTENCY' in f or 'AVG_PAYMENT' in f],\n",
    "    'Credit Utilization': [f for f in new_features if 'CREDIT' in f or 'UTILIZATION' in f or 'BUFFER' in f],\n",
    "    'Behavioral Classification': [f for f in new_features if 'BEHAVIOR' in f or 'SEGMENT' in f or 'RISK' in f]\n",
    "}\n",
    "\n",
    "print(f'\\\\nüìã NEW FEATURES BY CATEGORY:')\n",
    "for category, features in feature_categories.items():\n",
    "    print(f'   {category}: {len(features)} features')\n",
    "    for feature in sorted(features)[:3]:  # Show first 3 features per category\n",
    "        print(f'      - {feature}')\n",
    "    if len(features) > 3:\n",
    "        print(f'      ... and {len(features)-3} more')\n",
    "\n",
    "# Feature quality validation\n",
    "print(f'\\\\nüîç FEATURE QUALITY VALIDATION:')\n",
    "\n",
    "# Check for features with too many missing values\n",
    "missing_features = []\n",
    "for feature in new_features:\n",
    "    if feature in df_features.columns:\n",
    "        missing_count = df_features.filter(col(feature).isNull()).count()\n",
    "        if missing_count > 0:\n",
    "            missing_pct = missing_count / df_features.count() * 100\n",
    "            missing_features.append((feature, missing_count, missing_pct))\n",
    "\n",
    "if missing_features:\n",
    "    print(f'   ‚ö†Ô∏è  Features with missing values: {len(missing_features)}')\n",
    "    for feature, count, pct in missing_features[:5]:  # Show top 5\n",
    "        print(f'      {feature}: {count:,} ({pct:.1f}%)')\n",
    "else:\n",
    "    print(f'   ‚úÖ No missing values in engineered features')\n",
    "\n",
    "# Check for features with zero variance\n",
    "zero_variance_features = []\n",
    "for feature in new_features:\n",
    "    if feature in df_features.columns:\n",
    "        try:\n",
    "            variance = df_features.select(variance(feature)).collect()[0][0]\n",
    "            if variance is not None and variance < 1e-10:\n",
    "                zero_variance_features.append(feature)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "if zero_variance_features:\n",
    "    print(f'   ‚ö†Ô∏è  Features with zero/low variance: {len(zero_variance_features)}')\n",
    "    for feature in zero_variance_features[:5]:\n",
    "        print(f'      - {feature}')\n",
    "else:\n",
    "    print(f'   ‚úÖ All features have sufficient variance')\n",
    "\n",
    "# Feature correlation with target\n",
    "if 'default payment next month' in df_features.columns:\n",
    "    print(f'\\\\nüéØ TOP FEATURES BY CORRELATION WITH TARGET:')\n",
    "    \n",
    "    feature_correlations = []\n",
    "    \n",
    "    # Calculate correlations for numeric features\n",
    "    numeric_features = [f for f in new_features if f in df_features.columns and \n",
    "                       dict(df_features.dtypes)[f] in ['int', 'bigint', 'double', 'float']]\n",
    "    \n",
    "    for feature in numeric_features[:15]:  # Limit to prevent long computation\n",
    "        try:\n",
    "            assembler = VectorAssembler(inputCols=[feature, 'default payment next month'], outputCol='features')\n",
    "            df_corr = assembler.transform(df_features).select('features')\n",
    "            correlation_matrix = Correlation.corr(df_corr, 'features').head()[0]\n",
    "            correlation_value = float(correlation_matrix.toArray()[0, 1])\n",
    "            feature_correlations.append((feature, correlation_value))\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    # Sort by absolute correlation\n",
    "    feature_correlations.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "    \n",
    "    print(f'   {'Feature':<30} {'Correlation':<12} {'Strength':<12}')\n",
    "    print('   ' + '-' * 60)\n",
    "    \n",
    "    for feature, corr in feature_correlations[:10]:  # Top 10\n",
    "        abs_corr = abs(corr)\n",
    "        if abs_corr >= 0.3:\n",
    "            strength = 'Strong'\n",
    "        elif abs_corr >= 0.1:\n",
    "            strength = 'Moderate'  \n",
    "        elif abs_corr >= 0.05:\n",
    "            strength = 'Weak'\n",
    "        else:\n",
    "            strength = 'Very Weak'\n",
    "        \n",
    "        print(f'   {feature:<30} {corr:<12.4f} {strength:<12}')\n",
    "\n",
    "print(f'\\\\nüí° FEATURE ENGINEERING INSIGHTS:')\n",
    "insights = [\n",
    "    f'Created {len(new_features)} temporal and behavioral features',\n",
    "    f'Enhanced dataset from {len(original_columns)} to {len(all_columns)} dimensions',\n",
    "    f'Temporal features capture 6-month payment behavior patterns',\n",
    "    f'Risk scoring provides business-interpretable customer assessment',\n",
    "    f'Features ready for advanced machine learning modeling'\n",
    "]\n",
    "\n",
    "for insight in insights:\n",
    "    print(f'   ‚úÖ {insight}')\n",
    "\n",
    "print(f'\\\\n‚úÖ FEATURE ENGINEERING PHASE COMPLETED SUCCESSFULLY')\n",
    "print(f'üìÅ Proceed to notebook: 04_visualization_analysis.ipynb')\n",
    "\n",
    "# Save enhanced dataset with all engineered features\n",
    "print('üíæ SAVING ENHANCED DATASET WITH ENGINEERED FEATURES')\n",
    "print('=' * 60)\n",
    "\n",
    "try:\n",
    "    # Save complete enhanced dataset\n",
    "    df_features.write.mode('overwrite').parquet('../data/processed/03_enhanced_features.parquet')\n",
    "    \n",
    "    print(f'‚úÖ Enhanced dataset saved successfully')\n",
    "    print(f'   üìÅ Location: ../data/processed/03_enhanced_features.parquet')\n",
    "    print(f'   üìä Records: {df_features.count():,}')\n",
    "    print(f'   üìã Total columns: {len(df_features.columns)}')\n",
    "    print(f'   ‚öôÔ∏è  New features: {len(new_features)}')\n",
    "    \n",
    "    # Save feature documentation\n",
    "    feature_docs = {\n",
    "        'original_features': len(original_columns),\n",
    "        'engineered_features': len(new_features), \n",
    "        'total_features': len(df_features.columns),\n",
    "        'feature_categories': {k: len(v) for k, v in feature_categories.items()},\n",
    "        'created_date': '2025-06-20 16:12:16 UTC',\n",
    "        'analyst': 'ardzz'\n",
    "    }\n",
    "    \n",
    "    print(f'\\\\nüìã Feature documentation created')\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f'‚ö†Ô∏è  Could not save enhanced dataset: {e}')\n",
    "\n",
    "# Display final feature summary\n",
    "print(f'\\\\nüéØ READY FOR ADVANCED ANALYSIS:')\n",
    "print(f'   Next Phase: Visualization and Business Intelligence')\n",
    "print(f'   Enhanced features: Payment trends, temporal patterns, risk scoring')\n",
    "print(f'   Business value: Comprehensive customer behavior analysis')\n",
    "\n",
    "print(f'\\\\nüöÄ Feature Engineering Complete - Ready for Phase 4: Visualization Analysis')"
   ],
   "id": "127ee270367d7153"
  }
 ]
}
