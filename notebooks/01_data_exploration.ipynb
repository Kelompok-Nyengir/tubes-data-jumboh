{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. Comprehensive Data Exploration\n",
    "\n",
    "**Credit Card Default Analysis - Data Exploration Phase**\n",
    "\n",
    "## üìã Notebook Objectives\n",
    "\n",
    "1. **Dataset Overview**: Comprehensive understanding of structure and content\n",
    "2. **Research Variable Mapping**: Document all 23 explanatory variables (X1-X23)\n",
    "3. **Data Quality Assessment**: Identify missing values, outliers, and inconsistencies\n",
    "4. **Statistical Analysis**: Descriptive statistics and distributions\n",
    "5. **Initial Pattern Discovery**: Preliminary insights into default patterns\n",
    "\n",
    "## üéØ Expected Outcomes\n",
    "- Complete understanding of dataset structure\n",
    "- Research-standard variable documentation\n",
    "- Data quality report\n",
    "- Initial insights for subsequent analysis phases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced setup for data exploration\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../src')\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Import custom modules\n",
    "from src.data_processing import CreditCardDataProcessor\n",
    "from src.visualization import CreditCardVisualizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"Set2\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üìä CREDIT CARD DEFAULT ANALYSIS - DATA EXPLORATION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"üìÖ Analysis Date: 2025-06-20 15:56:05 UTC\")\n",
    "print(f\"üë§ Analyst: ardzz\")\n",
    "print(f\"üìù Phase: 1 of 5 - Exploratory Data Analysis\")\n",
    "print(f\"üîó Repository: Kelompok-Nyengir/tubes-data-jumboh\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CreditCardDataExploration\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(f\"‚úÖ Spark Session initialized successfully\")\n",
    "print(f\"   Spark Version: {spark.version}\")\n",
    "print(f\"   Spark UI: {spark.sparkContext.uiWebUrl}\")\n",
    "\n",
    "# Initialize data processor and visualizer\n",
    "processor = CreditCardDataProcessor(spark)\n",
    "visualizer = CreditCardVisualizer()\n",
    "\n",
    "print(f\"‚úÖ Custom modules initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Initial Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "print(\"üìÇ Loading credit card default dataset...\")\n",
    "\n",
    "try:\n",
    "    # Try primary path\n",
    "    df = processor.load_data(\"../data/sample.csv\")\n",
    "except:\n",
    "    try:\n",
    "        # Try alternative path\n",
    "        df = processor.load_data(\"../sample.csv\")\n",
    "    except:\n",
    "        # Try current directory\n",
    "        df = processor.load_data(\"sample.csv\")\n",
    "\n",
    "print(f\"‚úÖ Dataset loaded successfully\")\n",
    "\n",
    "# Basic dataset information\n",
    "row_count = df.count()\n",
    "col_count = len(df.columns)\n",
    "\n",
    "print(f\"\\nüìà DATASET OVERVIEW:\")\n",
    "print(f\"   Rows: {row_count:,}\")\n",
    "print(f\"   Columns: {col_count}\")\n",
    "print(f\"   Total data points: {row_count * col_count:,}\")\n",
    "\n",
    "# Display column information\n",
    "print(f\"\\nüìã COLUMN INFORMATION:\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research Variable Documentation (X1-X23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate schema and create research variable mapping\n",
    "print(\"üìã RESEARCH VARIABLE VALIDATION AND MAPPING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Validate schema against research variables\n",
    "schema_valid = processor.validate_schema(df)\n",
    "\n",
    "# Create comprehensive variable mapping\n",
    "variable_mapping = {\n",
    "    # Demographics (X1-X5)\n",
    "    'LIMIT_BAL': 'X1',    # Amount of given credit (NT dollar)\n",
    "    'SEX': 'X2',          # Gender (1=male, 2=female)\n",
    "    'EDUCATION': 'X3',    # Education (1=grad school, 2=university, 3=high school, 4=others)\n",
    "    'MARRIAGE': 'X4',     # Marital status (1=married, 2=single, 3=others)\n",
    "    'AGE': 'X5',          # Age (year)\n",
    "    \n",
    "    # Payment History (X6-X11) - September 2005 to April 2005\n",
    "    'PAY_0': 'X6',        # Repayment status in September 2005\n",
    "    'PAY_2': 'X7',        # Repayment status in August 2005\n",
    "    'PAY_3': 'X8',        # Repayment status in July 2005\n",
    "    'PAY_4': 'X9',        # Repayment status in June 2005\n",
    "    'PAY_5': 'X10',       # Repayment status in May 2005\n",
    "    'PAY_6': 'X11',       # Repayment status in April 2005\n",
    "    \n",
    "    # Bill Statements (X12-X17)\n",
    "    'BILL_AMT1': 'X12',   # Bill statement in September 2005\n",
    "    'BILL_AMT2': 'X13',   # Bill statement in August 2005\n",
    "    'BILL_AMT3': 'X14',   # Bill statement in July 2005\n",
    "    'BILL_AMT4': 'X15',   # Bill statement in June 2005\n",
    "    'BILL_AMT5': 'X16',   # Bill statement in May 2005\n",
    "    'BILL_AMT6': 'X17',   # Bill statement in April 2005\n",
    "    \n",
    "    # Payment Amounts (X18-X23)\n",
    "    'PAY_AMT1': 'X18',    # Amount paid in September 2005\n",
    "    'PAY_AMT2': 'X19',    # Amount paid in August 2005\n",
    "    'PAY_AMT3': 'X20',    # Amount paid in July 2005\n",
    "    'PAY_AMT4': 'X21',    # Amount paid in June 2005\n",
    "    'PAY_AMT5': 'X22',    # Amount paid in May 2005\n",
    "    'PAY_AMT6': 'X23',    # Amount paid in April 2005\n",
    "}\n",
    "\n",
    "# Display research variable mapping\n",
    "print(f\"\\nüìä RESEARCH VARIABLE MAPPING (X1-X23):\")\n",
    "print(f\"{'Original Column':<15} {'Research Var':<12} {'Category':<15} {'Available':<10}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "categories = {\n",
    "    'Demographics': ['X1', 'X2', 'X3', 'X4', 'X5'],\n",
    "    'Payment History': ['X6', 'X7', 'X8', 'X9', 'X10', 'X11'],\n",
    "    'Bill Statements': ['X12', 'X13', 'X14', 'X15', 'X16', 'X17'],\n",
    "    'Payment Amounts': ['X18', 'X19', 'X20', 'X21', 'X22', 'X23']\n",
    "}\n",
    "\n",
    "for category, vars_list in categories.items():\n",
    "    for research_var in vars_list:\n",
    "        # Find original column name\n",
    "        orig_col = None\n",
    "        for orig, res in variable_mapping.items():\n",
    "            if res == research_var:\n",
    "                orig_col = orig\n",
    "                break\n",
    "        \n",
    "        if orig_col:\n",
    "            available = \"‚úÖ Yes\" if orig_col in df.columns else \"‚ùå No\"\n",
    "            print(f\"{orig_col:<15} {research_var:<12} {category:<15} {available:<10}\")\n",
    "\n",
    "# Payment status codes\n",
    "payment_status_codes = {\n",
    "    -2: 'No consumption',\n",
    "    -1: 'Pay duly', \n",
    "    0: 'Use of revolving credit',\n",
    "    1: 'Payment delay for one month',\n",
    "    2: 'Payment delay for two months',\n",
    "    3: 'Payment delay for three months',\n",
    "    4: 'Payment delay for four months',\n",
    "    5: 'Payment delay for five months',\n",
    "    6: 'Payment delay for six months',\n",
    "    7: 'Payment delay for seven months',\n",
    "    8: 'Payment delay for eight months',\n",
    "    9: 'Payment delay for nine months and above'\n",
    "}\n",
    "\n",
    "print(f\"\\nüî¢ PAYMENT STATUS CODES (X6-X11):\")\n",
    "for code, meaning in payment_status_codes.items():\n",
    "    print(f\"   {code:2d}: {meaning}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive data quality assessment\n",
    "print(\"üîç COMPREHENSIVE DATA QUALITY ASSESSMENT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Run quality assessment using processor\n",
    "quality_report = processor.quality_assessment(df)\n",
    "\n",
    "print(f\"\\nüìä QUALITY SUMMARY:\")\n",
    "print(f\"   Total Records: {quality_report['total_records']:,}\")\n",
    "print(f\"   Total Columns: {quality_report['total_columns']}\")\n",
    "print(f\"   Missing Values: {len(quality_report['missing_values'])} columns affected\")\n",
    "print(f\"   Duplicate Records: {quality_report['duplicates']:,}\")\n",
    "\n",
    "# Detailed column analysis\n",
    "print(f\"\\nüìã DETAILED COLUMN ANALYSIS:\")\n",
    "print(f\"{'Column':<15} {'Type':<12} {'Nulls':<8} {'Unique':<8} {'Min':<12} {'Max':<12}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "for col_name in df.columns:\n",
    "    col_type = dict(df.dtypes)[col_name]\n",
    "    \n",
    "    # Count nulls\n",
    "    null_count = df.filter(col(col_name).isNull()).count()\n",
    "    \n",
    "    # Count unique values (for small categorical variables)\n",
    "    if col_name in ['SEX', 'EDUCATION', 'MARRIAGE'] or 'PAY_' in col_name:\n",
    "        unique_count = df.select(col_name).distinct().count()\n",
    "    else:\n",
    "        unique_count = \"N/A\"\n",
    "    \n",
    "    # Get min/max for numeric columns\n",
    "    if col_type in ['int', 'bigint', 'double', 'float']:\n",
    "        stats = df.select(min(col_name).alias('min'), max(col_name).alias('max')).collect()[0]\n",
    "        min_val = f\"{stats['min']:,.0f}\" if stats['min'] is not None else \"NULL\"\n",
    "        max_val = f\"{stats['max']:,.0f}\" if stats['max'] is not None else \"NULL\"\n",
    "    else:\n",
    "        min_val = \"N/A\"\n",
    "        max_val = \"N/A\"\n",
    "    \n",
    "    print(f\"{col_name:<15} {col_type:<12} {null_count:<8} {unique_count:<8} {min_val:<12} {max_val:<12}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Variable Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variable comprehensive analysis\n",
    "print(\"üéØ TARGET VARIABLE ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "target_col = \"default payment next month\"\n",
    "\n",
    "if target_col in df.columns:\n",
    "    # Basic distribution\n",
    "    target_dist = df.groupBy(target_col).count().orderBy(target_col)\n",
    "    target_data = target_dist.collect()\n",
    "    \n",
    "    total_records = sum([row['count'] for row in target_data])\n",
    "    \n",
    "    print(f\"\\nüìä TARGET VARIABLE DISTRIBUTION:\")\n",
    "    print(f\"{'Value':<15} {'Label':<15} {'Count':<10} {'Percentage':<12}\")\n",
    "    print(\"-\" * 55)\n",
    "    \n",
    "    for row in target_data:\n",
    "        value = row[target_col]\n",
    "        count = row['count']\n",
    "        percentage = count / total_records * 100\n",
    "        label = \"No Default\" if value == 0 else \"Default\"\n",
    "        \n",
    "        print(f\"{value:<15} {label:<15} {count:<10,} {percentage:<12.2f}%\")\n",
    "    \n",
    "    # Calculate class balance\n",
    "    default_count = df.filter(col(target_col) == 1).count()\n",
    "    no_default_count = df.filter(col(target_col) == 0).count()\n",
    "    default_rate = default_count / total_records * 100\n",
    "    \n",
    "    print(f\"\\nüîÑ CLASS BALANCE ANALYSIS:\")\n",
    "    print(f\"   Default Rate: {default_rate:.2f}%\")\n",
    "    print(f\"   Class Ratio (No Default : Default): {no_default_count/default_count:.2f} : 1\")\n",
    "    \n",
    "    if default_rate < 10:\n",
    "        print(f\"   ‚ö†Ô∏è  Imbalanced dataset - Consider resampling techniques\")\n",
    "    elif default_rate > 40:\n",
    "        print(f\"   ‚ö†Ô∏è  High default rate - Review data quality\")\n",
    "    else:\n",
    "        print(f\"   ‚úÖ Reasonable class balance\")\n",
    "        \n",
    "else:\n",
    "    print(f\"‚ùå Target variable '{target_col}' not found in dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demographic Variables Analysis (X1-X5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive demographic analysis\n",
    "print(\"üë• DEMOGRAPHIC VARIABLES ANALYSIS (X1-X5)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# X1: Credit Limit Analysis\n",
    "if 'LIMIT_BAL' in df.columns:\n",
    "    print(f\"\\nüí≥ X1 - CREDIT LIMIT (LIMIT_BAL) ANALYSIS:\")\n",
    "    \n",
    "    limit_stats = df.select(\n",
    "        min('LIMIT_BAL').alias('min'),\n",
    "        max('LIMIT_BAL').alias('max'),\n",
    "        avg('LIMIT_BAL').alias('mean'),\n",
    "        expr('percentile_approx(LIMIT_BAL, 0.5)').alias('median'),\n",
    "        stddev('LIMIT_BAL').alias('std')\n",
    "    ).collect()[0]\n",
    "    \n",
    "    print(f\"   Min: NT$ {limit_stats['min']:,.0f}\")\n",
    "    print(f\"   Max: NT$ {limit_stats['max']:,.0f}\")\n",
    "    print(f\"   Mean: NT$ {limit_stats['mean']:,.0f}\")\n",
    "    print(f\"   Median: NT$ {limit_stats['median']:,.0f}\")\n",
    "    print(f\"   Std Dev: NT$ {limit_stats['std']:,.0f}\")\n",
    "    \n",
    "    # Credit limit ranges\n",
    "    credit_ranges = df.withColumn(\n",
    "        \"credit_range\",\n",
    "        when(col(\"LIMIT_BAL\") < 50000, \"< 50K\")\n",
    "        .when(col(\"LIMIT_BAL\") < 100000, \"50K-100K\")\n",
    "        .when(col(\"LIMIT_BAL\") < 200000, \"100K-200K\")\n",
    "        .when(col(\"LIMIT_BAL\") < 500000, \"200K-500K\")\n",
    "        .otherwise(\">= 500K\")\n",
    "    ).groupBy(\"credit_range\").count().orderBy(\"count\", ascending=False)\n",
    "    \n",
    "    print(f\"\\n   Credit Limit Distribution:\")\n",
    "    for row in credit_ranges.collect():\n",
    "        count = row['count']\n",
    "        percentage = count / df.count() * 100\n",
    "        print(f\"      {row['credit_range']}: {count:,} ({percentage:.1f}%)\")\n",
    "\n",
    "# X2: Gender Analysis\n",
    "if 'SEX' in df.columns:\n",
    "    print(f\"\\nüë§ X2 - GENDER (SEX) ANALYSIS:\")\n",
    "    \n",
    "    gender_dist = df.groupBy('SEX').count().orderBy('SEX')\n",
    "    total_count = df.count()\n",
    "    \n",
    "    for row in gender_dist.collect():\n",
    "        gender_code = row['SEX']\n",
    "        count = row['count']\n",
    "        percentage = count / total_count * 100\n",
    "        gender_label = \"Male\" if gender_code == 1 else \"Female\" if gender_code == 2 else f\"Unknown ({gender_code})\"\n",
    "        print(f\"   {gender_label}: {count:,} ({percentage:.1f}%)\")\n",
    "\n",
    "# X3: Education Analysis\n",
    "if 'EDUCATION' in df.columns:\n",
    "    print(f\"\\nüéì X3 - EDUCATION ANALYSIS:\")\n",
    "    \n",
    "    edu_mapping = {1: \"Graduate School\", 2: \"University\", 3: \"High School\", 4: \"Others\"}\n",
    "    edu_dist = df.groupBy('EDUCATION').count().orderBy('EDUCATION')\n",
    "    \n",
    "    for row in edu_dist.collect():\n",
    "        edu_code = row['EDUCATION']\n",
    "        count = row['count']\n",
    "        percentage = count / total_count * 100\n",
    "        edu_label = edu_mapping.get(edu_code, f\"Code {edu_code}\")\n",
    "        print(f\"   {edu_label}: {count:,} ({percentage:.1f}%)\")\n",
    "\n",
    "# X4: Marriage Analysis\n",
    "if 'MARRIAGE' in df.columns:\n",
    "    print(f\"\\nüíí X4 - MARITAL STATUS (MARRIAGE) ANALYSIS:\")\n",
    "    \n",
    "    marriage_mapping = {1: \"Married\", 2: \"Single\", 3: \"Others\"}\n",
    "    marriage_dist = df.groupBy('MARRIAGE').count().orderBy('MARRIAGE')\n",
    "    \n",
    "    for row in marriage_dist.collect():\n",
    "        marriage_code = row['MARRIAGE']\n",
    "        count = row['count']\n",
    "        percentage = count / total_count * 100\n",
    "        marriage_label = marriage_mapping.get(marriage_code, f\"Code {marriage_code}\")\n",
    "        print(f\"   {marriage_label}: {count:,} ({percentage:.1f}%)\")\n",
    "\n",
    "# X5: Age Analysis\n",
    "if 'AGE' in df.columns:\n",
    "    print(f\"\\nüìÖ X5 - AGE ANALYSIS:\")\n",
    "    \n",
    "    age_stats = df.select(\n",
    "        min('AGE').alias('min'),\n",
    "        max('AGE').alias('max'),\n",
    "        avg('AGE').alias('mean'),\n",
    "        expr('percentile_approx(AGE, 0.5)').alias('median'),\n",
    "        stddev('AGE').alias('std')\n",
    "    ).collect()[0]\n",
    "    \n",
    "    print(f\"   Min Age: {age_stats['min']:.0f} years\")\n",
    "    print(f\"   Max Age: {age_stats['max']:.0f} years\")\n",
    "    print(f\"   Mean Age: {age_stats['mean']:.1f} years\")\n",
    "    print(f\"   Median Age: {age_stats['median']:.0f} years\")\n",
    "    print(f\"   Std Dev: {age_stats['std']:.1f} years\")\n",
    "    \n",
    "    # Age groups\n",
    "    age_groups = df.withColumn(\n",
    "        \"age_group\",\n",
    "        when(col(\"AGE\") < 30, \"< 30\")\n",
    "        .when(col(\"AGE\") < 40, \"30-39\")\n",
    "        .when(col(\"AGE\") < 50, \"40-49\")\n",
    "        .when(col(\"AGE\") < 60, \"50-59\")\n",
    "        .otherwise(\">= 60\")\n",
    "    ).groupBy(\"age_group\").count().orderBy(\"count\", ascending=False)\n",
    "    \n",
    "    print(f\"\\n   Age Group Distribution:\")\n",
    "    for row in age_groups.collect():\n",
    "        count = row['count']\n",
    "        percentage = count / df.count() * 100\n",
    "        print(f\"      {row['age_group']}: {count:,} ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Payment History Analysis (X6-X11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive payment history analysis\n",
    "print(\"üí≥ PAYMENT HISTORY ANALYSIS (X6-X11)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "pay_status_cols = ['PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6']\n",
    "months = ['September 2005', 'August 2005', 'July 2005', 'June 2005', 'May 2005', 'April 2005']\n",
    "research_vars = ['X6', 'X7', 'X8', 'X9', 'X10', 'X11']\n",
    "\n",
    "print(f\"\\nüìä PAYMENT STATUS DISTRIBUTION BY MONTH:\")\n",
    "\n",
    "for i, (col_name, month, research_var) in enumerate(zip(pay_status_cols, months, research_vars)):\n",
    "    if col_name in df.columns:\n",
    "        print(f\"\\n   {research_var} - {month} ({col_name}):\")\n",
    "        \n",
    "        status_dist = df.groupBy(col_name).count().orderBy(col_name)\n",
    "        total_records = df.count()\n",
    "        \n",
    "        for row in status_dist.collect():\n",
    "            status_code = row[col_name]\n",
    "            count = row['count']\n",
    "            percentage = count / total_records * 100\n",
    "            \n",
    "            # Get meaning from payment status codes\n",
    "            meaning = payment_status_codes.get(status_code, f\"Unknown code: {status_code}\")\n",
    "            \n",
    "            print(f\"      Code {status_code:2d}: {count:5,} ({percentage:5.1f}%) - {meaning}\")\n",
    "\n",
    "# Payment status summary statistics\n",
    "print(f\"\\nüìà PAYMENT STATUS SUMMARY STATISTICS:\")\n",
    "print(f\"{'Month':<15} {'Research Var':<12} {'Mean':<8} {'Std':<8} {'Min':<6} {'Max':<6}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for i, (col_name, month, research_var) in enumerate(zip(pay_status_cols, months, research_vars)):\n",
    "    if col_name in df.columns:\n",
    "        stats = df.select(\n",
    "            avg(col_name).alias('mean'),\n",
    "            stddev(col_name).alias('std'),\n",
    "            min(col_name).alias('min'),\n",
    "            max(col_name).alias('max')\n",
    "        ).collect()[0]\n",
    "        \n",
    "        month_short = month.split()[0][:3]\n",
    "        print(f\"{month_short:<15} {research_var:<12} {stats['mean']:<8.2f} {stats['std']:<8.2f} {stats['min']:<6.0f} {stats['max']:<6.0f}\")\n",
    "\n",
    "# Payment behavior patterns\n",
    "print(f\"\\nüîç PAYMENT BEHAVIOR PATTERNS:\")\n",
    "\n",
    "# Count of customers with excellent payment history (all <= 0)\n",
    "excellent_condition = \" AND \".join([f\"{col} <= 0\" for col in pay_status_cols if col in df.columns])\n",
    "if excellent_condition:\n",
    "    excellent_count = df.filter(expr(excellent_condition)).count()\n",
    "    excellent_pct = excellent_count / df.count() * 100\n",
    "    print(f\"   Excellent payers (all months <= 0): {excellent_count:,} ({excellent_pct:.1f}%)\")\n",
    "\n",
    "# Count of customers with any payment delays (any > 0)\n",
    "delay_condition = \" OR \".join([f\"{col} > 0\" for col in pay_status_cols if col in df.columns])\n",
    "if delay_condition:\n",
    "    delay_count = df.filter(expr(delay_condition)).count()\n",
    "    delay_pct = delay_count / df.count() * 100\n",
    "    print(f\"   Customers with delays (any month > 0): {delay_count:,} ({delay_pct:.1f}%)\")\n",
    "\n",
    "# Count of customers with severe delays (any >= 3)\n",
    "severe_condition = \" OR \".join([f\"{col} >= 3\" for col in pay_status_cols if col in df.columns])\n",
    "if severe_condition:\n",
    "    severe_count = df.filter(expr(severe_condition)).count()\n",
    "    severe_pct = severe_count / df.count() * 100\n",
    "    print(f\"   Customers with severe delays (any month >= 3): {severe_count:,} ({severe_pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Financial Variables Analysis (X12-X23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive financial variables analysis\n",
    "print(\"üí∞ FINANCIAL VARIABLES ANALYSIS (X12-X23)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "bill_cols = ['BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6']\n",
    "pay_cols = ['PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']\n",
    "bill_research_vars = ['X12', 'X13', 'X14', 'X15', 'X16', 'X17']\n",
    "pay_research_vars = ['X18', 'X19', 'X20', 'X21', 'X22', 'X23']\n",
    "\n",
    "# Bill Statement Analysis (X12-X17)\n",
    "print(f\"\\nüìã BILL STATEMENT ANALYSIS (X12-X17):\")\n",
    "print(f\"{'Month':<15} {'Research Var':<12} {'Mean (NT$)':<15} {'Median (NT$)':<15} {'Std Dev':<15}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for i, (col_name, month, research_var) in enumerate(zip(bill_cols, months, bill_research_vars)):\n",
    "    if col_name in df.columns:\n",
    "        stats = df.select(\n",
    "            avg(col_name).alias('mean'),\n",
    "            expr(f'percentile_approx({col_name}, 0.5)').alias('median'),\n",
    "            stddev(col_name).alias('std')\n",
    "        ).collect()[0]\n",
    "        \n",
    "        month_short = month.split()[0][:3]\n",
    "        print(f\"{month_short:<15} {research_var:<12} {stats['mean']:<15,.0f} {stats['median']:<15,.0f} {stats['std']:<15,.0f}\")\n",
    "\n",
    "# Payment Amount Analysis (X18-X23)\n",
    "print(f\"\\nüí∏ PAYMENT AMOUNT ANALYSIS (X18-X23):\")\n",
    "print(f\"{'Month':<15} {'Research Var':<12} {'Mean (NT$)':<15} {'Median (NT$)':<15} {'Std Dev':<15}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for i, (col_name, month, research_var) in enumerate(zip(pay_cols, months, pay_research_vars)):\n",
    "    if col_name in df.columns:\n",
    "        stats = df.select(\n",
    "            avg(col_name).alias('mean'),\n",
    "            expr(f'percentile_approx({col_name}, 0.5)').alias('median'),\n",
    "            stddev(col_name).alias('std')\n",
    "        ).collect()[0]\n",
    "        \n",
    "        month_short = month.split()[0][:3]\n",
    "        print(f\"{month_short:<15} {research_var:<12} {stats['mean']:<15,.0f} {stats['median']:<15,.0f} {stats['std']:<15,.0f}\")\n",
    "\n",
    "# Payment Efficiency Analysis\n",
    "print(f\"\\nüìä PAYMENT EFFICIENCY ANALYSIS:\")\n",
    "print(f\"   (Payment Amount / Bill Amount ratio by month)\")\n",
    "print(f\"{'Month':<15} {'Avg Efficiency':<15} {'Median Efficiency':<15}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for i, (bill_col, pay_col, month) in enumerate(zip(bill_cols, pay_cols, months)):\n",
    "    if bill_col in df.columns and pay_col in df.columns:\n",
    "        # Calculate payment efficiency (avoid division by zero)\n",
    "        efficiency_df = df.withColumn(\n",
    "            \"efficiency\",\n",
    "            when(col(bill_col) > 0, col(pay_col) / col(bill_col)).otherwise(0)\n",
    "        )\n",
    "        \n",
    "        efficiency_stats = efficiency_df.select(\n",
    "            avg(\"efficiency\").alias('mean_eff'),\n",
    "            expr('percentile_approx(efficiency, 0.5)').alias('median_eff')\n",
    "        ).collect()[0]\n",
    "        \n",
    "        month_short = month.split()[0][:3]\n",
    "        print(f\"{month_short:<15} {efficiency_stats['mean_eff']:<15.3f} {efficiency_stats['median_eff']:<15.3f}\")\n",
    "\n",
    "# Financial Health Indicators\n",
    "print(f\"\\nüè• FINANCIAL HEALTH INDICATORS:\")\n",
    "\n",
    "# Average credit utilization\n",
    "if 'LIMIT_BAL' in df.columns and bill_cols[0] in df.columns:\n",
    "    utilization_df = df.withColumn(\n",
    "        \"utilization\",\n",
    "        when(col(\"LIMIT_BAL\") > 0, col(bill_cols[0]) / col(\"LIMIT_BAL\")).otherwise(0)\n",
    "    )\n",
    "    \n",
    "    avg_utilization = utilization_df.select(avg(\"utilization\")).collect()[0][0]\n",
    "    print(f\"   Average credit utilization (latest month): {avg_utilization:.3f} ({avg_utilization*100:.1f}%)\")\n",
    "\n",
    "# Zero payment analysis\n",
    "for i, (pay_col, month) in enumerate(zip(pay_cols, months)):\n",
    "    if pay_col in df.columns:\n",
    "        zero_payment_count = df.filter(col(pay_col) == 0).count()\n",
    "        zero_payment_pct = zero_payment_count / df.count() * 100\n",
    "        month_short = month.split()[0][:3]\n",
    "        print(f\"   Zero payments in {month_short}: {zero_payment_count:,} ({zero_payment_pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial correlation analysis with target variable\n",
    "print(\"üîó INITIAL CORRELATION ANALYSIS WITH TARGET\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if target_col in df.columns:\n",
    "    # Select numeric columns for correlation analysis\n",
    "    numeric_cols = []\n",
    "    for col_name, col_type in df.dtypes:\n",
    "        if col_type in ['int', 'bigint', 'double', 'float'] and col_name != 'ID':\n",
    "            numeric_cols.append(col_name)\n",
    "    \n",
    "    print(f\"\\nüìä CORRELATION WITH DEFAULT (top correlations):\")\n",
    "    print(f\"{'Variable':<15} {'Research Var':<12} {'Correlation':<12} {'Abs Corr':<10} {'Strength':<15}\")\n",
    "    print(\"-\" * 75)\n",
    "    \n",
    "    correlations = []\n",
    "    \n",
    "    for col_name in numeric_cols:\n",
    "        if col_name != target_col:\n",
    "            try:\n",
    "                # Calculate correlation using Spark\n",
    "                assembler = VectorAssembler(inputCols=[col_name, target_col], outputCol=\"features\")\n",
    "                df_vector = assembler.transform(df).select(\"features\")\n",
    "                correlation_matrix = Correlation.corr(df_vector, \"features\").head()[0]\n",
    "                correlation_value = float(correlation_matrix.toArray()[0, 1])\n",
    "                \n",
    "                # Get research variable name\n",
    "                research_var = variable_mapping.get(col_name, \"N/A\")\n",
    "                \n",
    "                correlations.append((col_name, research_var, correlation_value))\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   Error calculating correlation for {col_name}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    # Sort by absolute correlation value\n",
    "    correlations.sort(key=lambda x: abs(x[2]), reverse=True)\n",
    "    \n",
    "    # Display top correlations\n",
    "    for col_name, research_var, corr_value in correlations[:15]:  # Top 15\n",
    "        abs_corr = abs(corr_value)\n",
    "        \n",
    "        if abs_corr >= 0.3:\n",
    "            strength = \"Strong\"\n",
    "        elif abs_corr >= 0.1:\n",
    "            strength = \"Moderate\"\n",
    "        elif abs_corr >= 0.05:\n",
    "            strength = \"Weak\"\n",
    "        else:\n",
    "            strength = \"Very Weak\"\n",
    "        \n",
    "        print(f\"{col_name:<15} {research_var:<12} {corr_value:<12.4f} {abs_corr:<10.4f} {strength:<15}\")\n",
    "    \n",
    "    print(f\"\\nüéØ KEY CORRELATION INSIGHTS:\")\n",
    "    \n",
    "    # Find strongest positive and negative correlations\n",
    "    if correlations:\n",
    "        strongest_pos = max(correlations, key=lambda x: x[2] if x[2] > 0 else -1)\n",
    "        strongest_neg = min(correlations, key=lambda x: x[2] if x[2] < 0 else 1)\n",
    "        \n",
    "        print(f\"   Strongest positive correlation: {strongest_pos[0]} ({strongest_pos[1]}) = {strongest_pos[2]:.4f}\")\n",
    "        print(f\"   Strongest negative correlation: {strongest_neg[0]} ({strongest_neg[1]}) = {strongest_neg[2]:.4f}\")\n",
    "        \n",
    "        # Count correlations by strength\n",
    "        strong_corr = sum(1 for _, _, corr in correlations if abs(corr) >= 0.3)\n",
    "        moderate_corr = sum(1 for _, _, corr in correlations if 0.1 <= abs(corr) < 0.3)\n",
    "        weak_corr = sum(1 for _, _, corr in correlations if 0.05 <= abs(corr) < 0.1)\n",
    "        \n",
    "        print(f\"   Strong correlations (|r| >= 0.3): {strong_corr}\")\n",
    "        print(f\"   Moderate correlations (0.1 <= |r| < 0.3): {moderate_corr}\")\n",
    "        print(f\"   Weak correlations (0.05 <= |r| < 0.1): {weak_corr}\")\n",
    "\n",
    "else:\n",
    "    print(f\"‚ùå Target variable '{target_col}' not found - skipping correlation analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create preview visualizations\n",
    "print(\"üìä CREATING PREVIEW VISUALIZATIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Convert to pandas for visualization (sample if dataset is large)\n",
    "sample_size = min(10000, df.count())\n",
    "df_viz = visualizer.convert_spark_to_pandas(df, sample_size=sample_size)\n",
    "\n",
    "print(f\"Using {len(df_viz):,} records for visualization\")\n",
    "\n",
    "# Create basic visualizations\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Credit Card Default Analysis - Data Exploration Preview\\n' +\n",
    "            f'Analysis Date: 2025-06-20 15:56:05 UTC | Analyst: ardzz',\n",
    "            fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Target distribution\n",
    "if target_col in df_viz.columns:\n",
    "    target_counts = df_viz[target_col].value_counts()\n",
    "    colors = ['lightblue', 'lightcoral']\n",
    "    axes[0,0].pie(target_counts.values, labels=['No Default', 'Default'], \n",
    "                  autopct='%1.1f%%', colors=colors, startangle=90)\n",
    "    axes[0,0].set_title('Target Distribution')\n",
    "\n",
    "# 2. Age distribution\n",
    "if 'AGE' in df_viz.columns:\n",
    "    axes[0,1].hist(df_viz['AGE'], bins=25, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[0,1].set_title('Age Distribution (X5)')\n",
    "    axes[0,1].set_xlabel('Age')\n",
    "    axes[0,1].set_ylabel('Frequency')\n",
    "\n",
    "# 3. Credit limit distribution\n",
    "if 'LIMIT_BAL' in df_viz.columns:\n",
    "    axes[0,2].hist(df_viz['LIMIT_BAL']/1000, bins=30, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "    axes[0,2].set_title('Credit Limit Distribution (X1)')\n",
    "    axes[0,2].set_xlabel('Credit Limit (NT$ thousands)')\n",
    "    axes[0,2].set_ylabel('Frequency')\n",
    "\n",
    "# 4. Gender vs Default\n",
    "if 'SEX' in df_viz.columns and target_col in df_viz.columns:\n",
    "    gender_default = df_viz.groupby('SEX')[target_col].mean() * 100\n",
    "    gender_labels = gender_default.index.map({1: 'Male', 2: 'Female'})\n",
    "    axes[1,0].bar(gender_labels, gender_default.values, color=['lightblue', 'lightpink'])\n",
    "    axes[1,0].set_title('Default Rate by Gender (X2)')\n",
    "    axes[1,0].set_ylabel('Default Rate (%)')\n",
    "\n",
    "# 5. Education vs Default\n",
    "if 'EDUCATION' in df_viz.columns and target_col in df_viz.columns:\n",
    "    edu_mapping = {1: 'Grad', 2: 'Univ', 3: 'High', 4: 'Others'}\n",
    "    edu_default = df_viz.groupby('EDUCATION')[target_col].mean() * 100\n",
    "    edu_labels = [edu_mapping.get(edu, f'Code {edu}') for edu in edu_default.index]\n",
    "    axes[1,1].bar(edu_labels, edu_default.values, color='orange', alpha=0.7)\n",
    "    axes[1,1].set_title('Default Rate by Education (X3)')\n",
    "    axes[1,1].set_ylabel('Default Rate (%)')\n",
    "    axes[1,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 6. Payment status vs Default (latest month)\n",
    "if 'PAY_0' in df_viz.columns and target_col in df_viz.columns:\n",
    "    pay_default = df_viz.groupby('PAY_0')[target_col].mean() * 100\n",
    "    axes[1,2].bar(pay_default.index, pay_default.values, color='red', alpha=0.7)\n",
    "    axes[1,2].set_title('Default Rate by Latest Payment Status (X6)')\n",
    "    axes[1,2].set_xlabel('Payment Status Code')\n",
    "    axes[1,2].set_ylabel('Default Rate (%)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ Preview visualizations created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive exploration summary\n",
    "print(\"üìã DATA EXPLORATION SUMMARY REPORT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüìÖ ANALYSIS METADATA:\")\n",
    "print(f\"   Analysis Date: 2025-06-20 15:56:05 UTC\")\n",
    "print(f\"   Analyst: ardzz\")\n",
    "print(f\"   Repository: Kelompok-Nyengir/tubes-data-jumboh\")\n",
    "print(f\"   Phase: 1 of 5 - Data Exploration Complete\")\n",
    "\n",
    "print(f\"\\nüìä DATASET SUMMARY:\")\n",
    "print(f\"   Total Records: {df.count():,}\")\n",
    "print(f\"   Total Variables: {len(df.columns)}\")\n",
    "print(f\"   Research Variables (X1-X23): 23 explanatory + 1 target\")\n",
    "print(f\"   Time Period: April 2005 - September 2005 (6 months)\")\n",
    "print(f\"   Data Quality: {len(quality_report['missing_values'])} columns with missing values\")\n",
    "print(f\"   Duplicate Records: {quality_report['duplicates']:,}\")\n",
    "\n",
    "if target_col in df.columns:\n",
    "    default_rate = df.filter(col(target_col) == 1).count() / df.count() * 100\n",
    "    print(f\"   Default Rate: {default_rate:.2f}%\")\n",
    "\n",
    "print(f\"\\nüéØ KEY FINDINGS:\")\n",
    "\n",
    "# Variable availability\n",
    "available_research_vars = sum(1 for orig_col in variable_mapping.keys() if orig_col in df.columns)\n",
    "print(f\"   ‚úÖ Research Variables Available: {available_research_vars}/23 ({available_research_vars/23*100:.1f}%)\")\n",
    "\n",
    "# Data quality findings\n",
    "if quality_report['duplicates'] == 0:\n",
    "    print(f\"   ‚úÖ No duplicate records found\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è  {quality_report['duplicates']:,} duplicate records found\")\n",
    "\n",
    "if len(quality_report['missing_values']) == 0:\n",
    "    print(f\"   ‚úÖ No missing values found\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è  Missing values in {len(quality_report['missing_values'])} columns\")\n",
    "\n",
    "# Correlation insights\n",
    "if 'correlations' in locals() and correlations:\n",
    "    strong_correlations = [corr for _, _, corr in correlations if abs(corr) >= 0.3]\n",
    "    if strong_correlations:\n",
    "        print(f\"   üìà Strong correlations found: {len(strong_correlations)} variables with |r| >= 0.3\")\n",
    "    else:\n",
    "        print(f\"   üìä No strong correlations found (all |r| < 0.3)\")\n",
    "\n",
    "print(f\"\\nüîÑ NEXT STEPS:\")\n",
    "print(f\"   üìù Phase 2: Data Cleansing (notebook 02_data_cleansing.ipynb)\")\n",
    "print(f\"      ‚Ä¢ Handle missing values and outliers\")\n",
    "print(f\"      ‚Ä¢ Clean categorical variables\")\n",
    "print(f\"      ‚Ä¢ Validate data consistency\")\n",
    "print(f\"\\n   ‚öôÔ∏è Phase 3: Feature Engineering (notebook 03_feature_engineering.ipynb)\")\n",
    "print(f\"      ‚Ä¢ Create temporal features from 6-month payment history\")\n",
    "print(f\"      ‚Ä¢ Develop payment behavior indicators\")\n",
    "print(f\"      ‚Ä¢ Engineer risk scoring features\")\n",
    "print(f\"\\n   üìä Phase 4: Advanced Visualization (notebook 04_visualization_analysis.ipynb)\")\n",
    "print(f\"      ‚Ä¢ Create comprehensive dashboards\")\n",
    "print(f\"      ‚Ä¢ Temporal pattern analysis\")\n",
    "print(f\"      ‚Ä¢ Interactive visualizations\")\n",
    "print(f\"\\n   ü§ñ Phase 5: Machine Learning (notebook 05_machine_learning.ipynb)\")\n",
    "print(f\"      ‚Ä¢ Multiple algorithm implementation\")\n",
    "print(f\"      ‚Ä¢ Hyperparameter tuning\")\n",
    "print(f\"      ‚Ä¢ Model evaluation and selection\")\n",
    "\n",
    "print(f\"\\n‚úÖ DATA EXPLORATION PHASE COMPLETED SUCCESSFULLY\")\n",
    "print(f\"üìÅ Proceed to notebook: 02_data_cleansing.ipynb\")\n",
    "\n",
    "# Save exploration results for next phase\n",
    "print(f\"\\nüíæ Saving exploration results...\")\n",
    "try:\n",
    "    # Cache the DataFrame for next notebook\n",
    "    df.write.mode(\"overwrite\").parquet(\"../data/processed/01_exploration_cache.parquet\")\n",
    "    print(f\"‚úÖ Exploration cache saved successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Could not save cache: {e}\")\n",
    "\n",
    "print(f\"\\nüéØ Ready for Phase 2: Data Cleansing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up Spark session\n",
    "spark.stop()\n",
    "print(\"‚úÖ Spark session closed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
