{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03. Advanced Temporal Feature Engineering\n",
    "\n",
    "**Credit Card Default Analysis - Feature Engineering Phase**\n",
    "\n",
    "- **Analysis Date**: 2025-06-20 16:04:47 UTC\n",
    "- **Analyst**: ardzz\n",
    "- **Repository**: Kelompok-Nyengir/tubes-data-jumboh\n",
    "- **Phase**: 3 of 5 - Advanced Feature Creation\n",
    "\n",
    "## üìã Notebook Objectives\n",
    "\n",
    "1. **Temporal Feature Engineering**: Create 25+ features from 6-month payment history\n",
    "2. **Payment Behavior Analysis**: Develop behavioral indicators and trends\n",
    "3. **Credit Utilization Features**: Engineer credit usage and efficiency metrics\n",
    "4. **Risk Scoring Components**: Build comprehensive risk assessment features\n",
    "5. **Customer Segmentation**: Create business-oriented customer categories\n",
    "\n",
    "## üéØ Expected Outcomes\n",
    "- Rich feature set for machine learning models\n",
    "- Temporal insights into payment behaviors\n",
    "- Business-interpretable risk indicators\n",
    "- Enhanced predictive capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced setup for feature engineering\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../src')\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Import custom modules\n",
    "from feature_engineering import TemporalFeatureEngineer\n",
    "from visualization import CreditCardVisualizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"Set2\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"‚öôÔ∏è CREDIT CARD DEFAULT ANALYSIS - FEATURE ENGINEERING\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"üìÖ Analysis Date: 2025-06-20 16:04:47 UTC\")\n",
    "print(f\"üë§ Analyst: ardzz\")\n",
    "print(f\"üìù Phase: 3 of 5 - Advanced Feature Creation\")\n",
    "print(f\"üîó Repository: Kelompok-Nyengir/tubes-data-jumboh\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CreditCardFeatureEngineering\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(f\"‚úÖ Spark Session initialized successfully\")\n",
    "print(f\"   Spark Version: {spark.version}\")\n",
    "print(f\"   Spark UI: {spark.sparkContext.uiWebUrl}\")\n",
    "\n",
    "# Initialize feature engineer and visualizer\n",
    "feature_engineer = TemporalFeatureEngineer()\n",
    "visualizer = CreditCardVisualizer()\n",
    "\n",
    "print(f\"‚úÖ Custom modules initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Pre-Engineering Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cleaned dataset from previous phase\n",
    "print(\"üìÇ Loading cleaned dataset for feature engineering...\")\n",
    "\n",
    "try:\n",
    "    # Try to load cleaned data from previous phase\n",
    "    df_clean = spark.read.parquet(\"../data/processed/02_cleaned_data.parquet\")\n",
    "    print(f\"‚úÖ Loaded cleaned dataset from Phase 2\")\n",
    "except:\n",
    "    try:\n",
    "        # Fallback to exploration cache\n",
    "        df_clean = spark.read.parquet(\"../data/processed/01_exploration_cache.parquet\")\n",
    "        print(f\"‚ö†Ô∏è  Using exploration cache - applying basic cleaning\")\n",
    "        \n",
    "        # Apply basic cleaning\n",
    "        df_clean = df_clean.withColumn(\n",
    "            \"EDUCATION\", when(col(\"EDUCATION\").isin([0, 5, 6]), 4).otherwise(col(\"EDUCATION\"))\n",
    "        ).withColumn(\n",
    "            \"MARRIAGE\", when(col(\"MARRIAGE\") == 0, 3).otherwise(col(\"MARRIAGE\"))\n",
    "        )\n",
    "    except:\n",
    "        # Final fallback to original CSV with cleaning\n",
    "        from data_processing import CreditCardDataProcessor\n",
    "        processor = CreditCardDataProcessor(spark)\n",
    "        \n",
    "        try:\n",
    "            df_raw = processor.load_data(\"../data/sample.csv\")\n",
    "        except:\n",
    "            df_raw = processor.load_data(\"../sample.csv\")\n",
    "        \n",
    "        df_clean = processor.clean_data(df_raw)\n",
    "        print(f\"‚ö†Ô∏è  Loaded and cleaned original CSV\")\n",
    "\n",
    "# Dataset assessment\n",
    "print(f\"\\nüìä PRE-FEATURE ENGINEERING ASSESSMENT:\")\n",
    "print(f\"   Records: {df_clean.count():,}\")\n",
    "print(f\"   Columns: {len(df_clean.columns)}\")\n",
    "\n",
    "# Check for required columns\n",
    "required_columns = {\n",
    "    'demographic': ['LIMIT_BAL', 'SEX', 'EDUCATION', 'MARRIAGE', 'AGE'],\n",
    "    'payment_status': ['PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6'],\n",
    "    'bill_amounts': ['BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6'],\n",
    "    'payment_amounts': ['PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']\n",
    "}\n",
    "\n",
    "print(f\"\\nüìã REQUIRED COLUMNS AVAILABILITY:\")\n",
    "for category, cols in required_columns.items():\n",
    "    available = sum(1 for col in cols if col in df_clean.columns)\n",
    "    print(f\"   {category.title()}: {available}/{len(cols)} columns available\")\n",
    "    \n",
    "    missing = [col for col in cols if col not in df_clean.columns]\n",
    "    if missing:\n",
    "        print(f\"      Missing: {', '.join(missing)}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset ready for feature engineering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1: Payment Trend and Volatility Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create payment trend and volatility features\n",
    "print(\"üìà PHASE 1: PAYMENT TREND AND VOLATILITY FEATURES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Apply payment trend feature engineering\n",
    "df_features = feature_engineer.create_payment_trend_features(df_clean)\n",
    "\n",
    "# Analyze the created features\n",
    "trend_features = [\n",
    "    'PAYMENT_TREND_SLOPE', 'PAYMENT_STATUS_VOLATILITY', \n",
    "    'MAX_PAYMENT_DELAY', 'MIN_PAYMENT_DELAY', 'PAYMENT_DELAY_RANGE'\n",
    "]\n",
    "\n",
    "print(f\"\\nüìä PAYMENT TREND FEATURES ANALYSIS:\")\n",
    "print(f\"{'Feature':<25} {'Mean':<10} {'Std':<10} {'Min':<10} {'Max':<10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for feature in trend_features:\n",
    "    if feature in df_features.columns:\n",
    "        stats = df_features.select(\n",
    "            avg(feature).alias('mean'),\n",
    "            stddev(feature).alias('std'),\n",
    "            min(feature).alias('min'),\n",
    "            max(feature).alias('max')\n",
    "        ).collect()[0]\n",
    "        \n",
    "        print(f\"{feature:<25} {stats['mean']:<10.3f} {stats['std']:<10.3f} \"\n",
    "              f\"{stats['min']:<10.3f} {stats['max']:<10.3f}\")\n",
    "\n",
    "# Analyze payment trend slope distribution\n",
    "if 'PAYMENT_TREND_SLOPE' in df_features.columns:\n",
    "    print(f\"\\nüîç PAYMENT TREND SLOPE INTERPRETATION:\")\n",
    "    \n",
    "    # Categorize trend slopes\n",
    "    trend_categories = df_features.withColumn(\n",
    "        \"TREND_CATEGORY\",\n",
    "        when(col(\"PAYMENT_TREND_SLOPE\") > 1, \"Strongly Improving\")\n",
    "        .when(col(\"PAYMENT_TREND_SLOPE\") > 0.2, \"Improving\")\n",
    "        .when(col(\"PAYMENT_TREND_SLOPE\") > -0.2, \"Stable\")\n",
    "        .when(col(\"PAYMENT_TREND_SLOPE\") > -1, \"Deteriorating\")\n",
    "        .otherwise(\"Strongly Deteriorating\")\n",
    "    ).groupBy(\"TREND_CATEGORY\").count().orderBy(desc(\"count\"))\n",
    "    \n",
    "    total_customers = df_features.count()\n",
    "    print(f\"   Payment Trend Distribution:\")\n",
    "    for row in trend_categories.collect():\n",
    "        category = row['TREND_CATEGORY']\n",
    "        count = row['count']\n",
    "        percentage = count / total_customers * 100\n",
    "        print(f\"      {category}: {count:,} ({percentage:.1f}%)\")\n",
    "\n",
    "# Payment volatility analysis\n",
    "if 'PAYMENT_STATUS_VOLATILITY' in df_features.columns:\n",
    "    print(f\"\\nüìä PAYMENT VOLATILITY ANALYSIS:\")\n",
    "    \n",
    "    volatility_percentiles = df_features.select(\n",
    "        expr('percentile_approx(PAYMENT_STATUS_VOLATILITY, 0.25)').alias('q1'),\n",
    "        expr('percentile_approx(PAYMENT_STATUS_VOLATILITY, 0.5)').alias('median'),\n",
    "        expr('percentile_approx(PAYMENT_STATUS_VOLATILITY, 0.75)').alias('q3'),\n",
    "        expr('percentile_approx(PAYMENT_STATUS_VOLATILITY, 0.9)').alias('p90')\n",
    "    ).collect()[0]\n",
    "    \n",
    "    print(f\"   Volatility Percentiles:\")\n",
    "    print(f\"      25th percentile: {volatility_percentiles['q1']:.3f}\")\n",
    "    print(f\"      Median: {volatility_percentiles['median']:.3f}\")\n",
    "    print(f\"      75th percentile: {volatility_percentiles['q3']:.3f}\")\n",
    "    print(f\"      90th percentile: {volatility_percentiles['p90']:.3f}\")\n",
    "    \n",
    "    # High volatility customers\n",
    "    high_volatility_threshold = volatility_percentiles['p90']\n",
    "    high_volatility_count = df_features.filter(\n",
    "        col(\"PAYMENT_STATUS_VOLATILITY\") >= high_volatility_threshold\n",
    "    ).count()\n",
    "    \n",
    "    print(f\"\\n   High Volatility Customers (>90th percentile):\")\n",
    "    print(f\"      Count: {high_volatility_count:,} ({high_volatility_count/total_customers*100:.1f}%)\")\n",
    "    print(f\"      Threshold: {high_volatility_threshold:.3f}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Phase 1 completed: {len(trend_features)} trend and volatility features created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2: Temporal Segmentation Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temporal segmentation features\n",
    "print(\"üïí PHASE 2: TEMPORAL SEGMENTATION FEATURES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Apply temporal segmentation feature engineering\n",
    "df_features = feature_engineer.create_temporal_segmentation_features(df_features)\n",
    "\n",
    "# Analyze temporal segmentation features\n",
    "temporal_features = [\n",
    "    'RECENT_AVG_DELAY', 'HISTORICAL_AVG_DELAY', \n",
    "    'PAYMENT_IMPROVEMENT_SCORE', 'RECOVERY_INSTANCES'\n",
    "]\n",
    "\n",
    "print(f\"\\nüìä TEMPORAL SEGMENTATION FEATURES ANALYSIS:\")\n",
    "print(f\"{'Feature':<25} {'Mean':<10} {'Std':<10} {'Min':<10} {'Max':<10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for feature in temporal_features:\n",
    "    if feature in df_features.columns:\n",
    "        stats = df_features.select(\n",
    "            avg(feature).alias('mean'),\n",
    "            stddev(feature).alias('std'),\n",
    "            min(feature).alias('min'),\n",
    "            max(feature).alias('max')\n",
    "        ).collect()[0]\n",
    "        \n",
    "        print(f\"{feature:<25} {stats['mean']:<10.3f} {stats['std']:<10.3f} \"\n",
    "              f\"{stats['min']:<10.3f} {stats['max']:<10.3f}\")\n",
    "\n",
    "# Payment improvement analysis\n",
    "if 'PAYMENT_IMPROVEMENT_SCORE' in df_features.columns:\n",
    "    print(f\"\\nüìà PAYMENT IMPROVEMENT ANALYSIS:\")\n",
    "    \n",
    "    # Categorize improvement scores\n",
    "    improvement_categories = df_features.withColumn(\n",
    "        \"IMPROVEMENT_CATEGORY\",\n",
    "        when(col(\"PAYMENT_IMPROVEMENT_SCORE\") > 2, \"Significant Improvement\")\n",
    "        .when(col(\"PAYMENT_IMPROVEMENT_SCORE\") > 0.5, \"Moderate Improvement\")\n",
    "        .when(col(\"PAYMENT_IMPROVEMENT_SCORE\") > -0.5, \"Stable\")\n",
    "        .when(col(\"PAYMENT_IMPROVEMENT_SCORE\") > -2, \"Moderate Deterioration\")\n",
    "        .otherwise(\"Significant Deterioration\")\n",
    "    ).groupBy(\"IMPROVEMENT_CATEGORY\").count().orderBy(desc(\"count\"))\n",
    "    \n",
    "    print(f\"   Payment Improvement Distribution:\")\n",
    "    for row in improvement_categories.collect():\n",
    "        category = row['IMPROVEMENT_CATEGORY']\n",
    "        count = row['count']\n",
    "        percentage = count / df_features.count() * 100\n",
    "        print(f\"      {category}: {count:,} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Correlation with default\n",
    "    if 'default payment next month' in df_features.columns:\n",
    "        # Calculate correlation\n",
    "        assembler = VectorAssembler(\n",
    "            inputCols=[\"PAYMENT_IMPROVEMENT_SCORE\", \"default payment next month\"], \n",
    "            outputCol=\"features\"\n",
    "        )\n",
    "        df_corr = assembler.transform(df_features).select(\"features\")\n",
    "        correlation_matrix = Correlation.corr(df_corr, \"features\").head()[0]\n",
    "        correlation_value = float(correlation_matrix.toArray()[0, 1])\n",
    "        \n",
    "        print(f\"\\n   Correlation with Default: {correlation_value:.4f}\")\n",
    "        if abs(correlation_value) > 0.1:\n",
    "            direction = \"negative\" if correlation_value < 0 else \"positive\"\n",
    "            strength = \"strong\" if abs(correlation_value) > 0.3 else \"moderate\" if abs(correlation_value) > 0.1 else \"weak\"\n",
    "            print(f\"      {strength.title()} {direction} correlation detected\")\n",
    "\n",
    "# Recovery instances analysis\n",
    "if 'RECOVERY_INSTANCES' in df_features.columns:\n",
    "    print(f\"\\nüîÑ RECOVERY INSTANCES ANALYSIS:\")\n",
    "    \n",
    "    recovery_distribution = df_features.groupBy(\"RECOVERY_INSTANCES\").count().orderBy(\"RECOVERY_INSTANCES\")\n",
    "    \n",
    "    print(f\"   Recovery Instances Distribution:\")\n",
    "    print(f\"   {'Instances':<12} {'Count':<10} {'Percentage':<12} {'Interpretation'}\")\n",
    "    print(\"-\" * 55)\n",
    "    \n",
    "    for row in recovery_distribution.collect():\n",
    "        instances = row['RECOVERY_INSTANCES']\n",
    "        count = row['count']\n",
    "        percentage = count / df_features.count() * 100\n",
    "        \n",
    "        if instances == 0:\n",
    "            interpretation = \"No recovery\"\n",
    "        elif instances <= 2:\n",
    "            interpretation = \"Limited recovery\"\n",
    "        elif instances <= 4:\n",
    "            interpretation = \"Good recovery\"\n",
    "        else:\n",
    "            interpretation = \"Excellent recovery\"\n",
    "        \n",
    "        print(f\"   {instances:<12} {count:<10,} {percentage:<12.1f}% {interpretation}\")\n",
    "    \n",
    "    # Average recovery by default status\n",
    "    if 'default payment next month' in df_features.columns:\n",
    "        avg_recovery_by_default = df_features.groupBy(\"default payment next month\") \\\n",
    "            .agg(avg(\"RECOVERY_INSTANCES\").alias(\"avg_recovery\")) \\\n",
    "            .orderBy(\"default payment next month\")\n",
    "        \n",
    "        print(f\"\\n   Average Recovery by Default Status:\")\n",
    "        for row in avg_recovery_by_default.collect():\n",
    "            default_status = \"No Default\" if row['default payment next month'] == 0 else \"Default\"\n",
    "            avg_recovery = row['avg_recovery']\n",
    "            print(f\"      {default_status}: {avg_recovery:.2f} recovery instances\")\n",
    "\n",
    "print(f\"\\n‚úÖ Phase 2 completed: {len(temporal_features)} temporal segmentation features created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 3: Bill Statement and Financial Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bill statement and financial features\n",
    "print(\"üí∞ PHASE 3: BILL STATEMENT AND FINANCIAL FEATURES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Apply bill statement feature engineering\n",
    "df_features = feature_engineer.create_bill_statement_features(df_features)\n",
    "\n",
    "# Analyze bill statement features\n",
    "bill_features = [\n",
    "    'BILL_TREND_SLOPE', 'BILL_AMOUNT_VOLATILITY', \n",
    "    'DEBT_ACCUMULATION_RATE', 'AVG_BILL_AMOUNT'\n",
    "]\n",
    "\n",
    "print(f\"\\nüìä BILL STATEMENT FEATURES ANALYSIS:\")\n",
    "print(f\"{'Feature':<25} {'Mean':<12} {'Std':<12} {'Min':<12} {'Max':<12}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for feature in bill_features:\n",
    "    if feature in df_features.columns:\n",
    "        stats = df_features.select(\n",
    "            avg(feature).alias('mean'),\n",
    "            stddev(feature).alias('std'),\n",
    "            min(feature).alias('min'),\n",
    "            max(feature).alias('max')\n",
    "        ).collect()[0]\n",
    "        \n",
    "        print(f\"{feature:<25} {stats['mean']:<12.2f} {stats['std']:<12.2f} \"\n",
    "              f\"{stats['min']:<12.2f} {stats['max']:<12.2f}\")\n",
    "\n",
    "# Bill trend analysis\n",
    "if 'BILL_TREND_SLOPE' in df_features.columns:\n",
    "    print(f\"\\nüìà BILL TREND ANALYSIS:\")\n",
    "    \n",
    "    # Categorize bill trends\n",
    "    bill_trend_categories = df_features.withColumn(\n",
    "        \"BILL_TREND_CATEGORY\",\n",
    "        when(col(\"BILL_TREND_SLOPE\") > 5000, \"Rapidly Increasing\")\n",
    "        .when(col(\"BILL_TREND_SLOPE\") > 1000, \"Increasing\")\n",
    "        .when(col(\"BILL_TREND_SLOPE\") > -1000, \"Stable\")\n",
    "        .when(col(\"BILL_TREND_SLOPE\") > -5000, \"Decreasing\")\n",
    "        .otherwise(\"Rapidly Decreasing\")\n",
    "    ).groupBy(\"BILL_TREND_CATEGORY\").count().orderBy(desc(\"count\"))\n",
    "    \n",
    "    print(f\"   Bill Trend Distribution:\")\n",
    "    for row in bill_trend_categories.collect():\n",
    "        category = row['BILL_TREND_CATEGORY']\n",
    "        count = row['count']\n",
    "        percentage = count / df_features.count() * 100\n",
    "        print(f\"      {category}: {count:,} ({percentage:.1f}%)\")\n",
    "\n",
    "# Debt accumulation analysis\n",
    "if 'DEBT_ACCUMULATION_RATE' in df_features.columns:\n",
    "    print(f\"\\nüìä DEBT ACCUMULATION ANALYSIS:\")\n",
    "    \n",
    "    # Analyze debt accumulation patterns\n",
    "    debt_percentiles = df_features.select(\n",
    "        expr('percentile_approx(DEBT_ACCUMULATION_RATE, 0.1)').alias('p10'),\n",
    "        expr('percentile_approx(DEBT_ACCUMULATION_RATE, 0.25)').alias('q1'),\n",
    "        expr('percentile_approx(DEBT_ACCUMULATION_RATE, 0.5)').alias('median'),\n",
    "        expr('percentile_approx(DEBT_ACCUMULATION_RATE, 0.75)').alias('q3'),\n",
    "        expr('percentile_approx(DEBT_ACCUMULATION_RATE, 0.9)').alias('p90')\n",
    "    ).collect()[0]\n",
    "    \n",
    "    print(f\"   Debt Accumulation Rate Percentiles:\")\n",
    "    print(f\"      10th percentile: {debt_percentiles['p10']:.3f}\")\n",
    "    print(f\"      25th percentile: {debt_percentiles['q1']:.3f}\")\n",
    "    print(f\"      Median: {debt_percentiles['median']:.3f}\")\n",
    "    print(f\"      75th percentile: {debt_percentiles['q3']:.3f}\")\n",
    "    print(f\"      90th percentile: {debt_percentiles['p90']:.3f}\")\n",
    "    \n",
    "    # High debt accumulation customers\n",
    "    high_debt_threshold = 0.5  # 50% increase\n",
    "    high_debt_count = df_features.filter(col(\"DEBT_ACCUMULATION_RATE\") > high_debt_threshold).count()\n",
    "    \n",
    "    print(f\"\\n   High Debt Accumulation (>50% increase):\")\n",
    "    print(f\"      Count: {high_debt_count:,} ({high_debt_count/df_features.count()*100:.1f}%)\")\n",
    "    \n",
    "    # Negative debt accumulation (debt reduction)\n",
    "    debt_reduction_count = df_features.filter(col(\"DEBT_ACCUMULATION_RATE\") < -0.1).count()\n",
    "    print(f\"\\n   Debt Reduction (>10% decrease):\")\n",
    "    print(f\"      Count: {debt_reduction_count:,} ({debt_reduction_count/df_features.count()*100:.1f}%)\")\n",
    "\n",
    "# Average bill amount analysis\n",
    "if 'AVG_BILL_AMOUNT' in df_features.columns and 'LIMIT_BAL' in df_features.columns:\n",
    "    print(f\"\\nüí≥ CREDIT UTILIZATION ANALYSIS:\")\n",
    "    \n",
    "    # Calculate basic credit utilization\n",
    "    utilization_stats = df_features.withColumn(\n",
    "        \"BASIC_UTILIZATION\",\n",
    "        when(col(\"LIMIT_BAL\") > 0, col(\"AVG_BILL_AMOUNT\") / col(\"LIMIT_BAL\")).otherwise(0)\n",
    "    ).select(\n",
    "        avg(\"BASIC_UTILIZATION\").alias('avg_util'),\n",
    "        expr('percentile_approx(BASIC_UTILIZATION, 0.5)').alias('median_util'),\n",
    "        expr('percentile_approx(BASIC_UTILIZATION, 0.9)').alias('p90_util')\n",
    "    ).collect()[0]\n",
    "    \n",
    "    print(f\"   Credit Utilization Statistics:\")\n",
    "    print(f\"      Average utilization: {utilization_stats['avg_util']:.3f} ({utilization_stats['avg_util']*100:.1f}%)\")\n",
    "    print(f\"      Median utilization: {utilization_stats['median_util']:.3f} ({utilization_stats['median_util']*100:.1f}%)\")\n",
    "    print(f\"      90th percentile: {utilization_stats['p90_util']:.3f} ({utilization_stats['p90_util']*100:.1f}%)\")\n",
    "    \n",
    "    # High utilization customers\n",
    "    high_util_count = df_features.filter(\n",
    "        (col(\"LIMIT_BAL\") > 0) & (col(\"AVG_BILL_AMOUNT\") / col(\"LIMIT_BAL\") > 0.8)\n",
    "    ).count()\n",
    "    \n",
    "    print(f\"\\n   High Utilization (>80%):\")\n",
    "    print(f\"      Count: {high_util_count:,} ({high_util_count/df_features.count()*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n‚úÖ Phase 3 completed: {len(bill_features)} bill statement and financial features created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 4: Payment Efficiency and Consistency Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create payment efficiency and consistency features\n",
    "print(\"‚ö° PHASE 4: PAYMENT EFFICIENCY AND CONSISTENCY FEATURES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Apply payment efficiency feature engineering\n",
    "df_features = feature_engineer.create_payment_efficiency_features(df_features)\n",
    "\n",
    "# Analyze payment efficiency features\n",
    "efficiency_features = [\n",
    "    'AVG_PAYMENT_EFFICIENCY', 'PAYMENT_EFFICIENCY_TREND',\n",
    "    'PAYMENT_CONSISTENCY_SCORE', 'AVG_PAYMENT_AMOUNT'\n",
    "]\n",
    "\n",
    "print(f\"\\nüìä PAYMENT EFFICIENCY FEATURES ANALYSIS:\")\n",
    "print(f\"{'Feature':<25} {'Mean':<12} {'Std':<12} {'Min':<12} {'Max':<12}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for feature in efficiency_features:\n",
    "    if feature in df_features.columns:\n",
    "        stats = df_features.select(\n",
    "            avg(feature).alias('mean'),\n",
    "            stddev(feature).alias('std'),\n",
    "            min(feature).alias('min'),\n",
    "            max(feature).alias('max')\n",
    "        ).collect()[0]\n",
    "        \n",
    "        print(f\"{feature:<25} {stats['mean']:<12.3f} {stats['std']:<12.3f} \"\n",
    "              f\"{stats['min']:<12.3f} {stats['max']:<12.3f}\")\n",
    "\n",
    "# Payment efficiency analysis\n",
    "if 'AVG_PAYMENT_EFFICIENCY' in df_features.columns:\n",
    "    print(f\"\\n‚ö° PAYMENT EFFICIENCY ANALYSIS:\")\n",
    "    \n",
    "    # Categorize payment efficiency\n",
    "    efficiency_categories = df_features.withColumn(\n",
    "        \"EFFICIENCY_CATEGORY\",\n",
    "        when(col(\"AVG_PAYMENT_EFFICIENCY\") >= 1.0, \"Full Payment\")\n",
    "        .when(col(\"AVG_PAYMENT_EFFICIENCY\") >= 0.8, \"High Efficiency\")\n",
    "        .when(col(\"AVG_PAYMENT_EFFICIENCY\") >= 0.5, \"Medium Efficiency\")\n",
    "        .when(col(\"AVG_PAYMENT_EFFICIENCY\") >= 0.2, \"Low Efficiency\")\n",
    "        .otherwise(\"Very Low Efficiency\")\n",
    "    ).groupBy(\"EFFICIENCY_CATEGORY\").count().orderBy(desc(\"count\"))\n",
    "    \n",
    "    print(f\"   Payment Efficiency Distribution:\")\n",
    "    for row in efficiency_categories.collect():\n",
    "        category = row['EFFICIENCY_CATEGORY']\n",
    "        count = row['count']\n",
    "        percentage = count / df_features.count() * 100\n",
    "        print(f\"      {category}: {count:,} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Efficiency vs default correlation\n",
    "    if 'default payment next month' in df_features.columns:\n",
    "        efficiency_by_default = df_features.groupBy(\"default payment next month\") \\\n",
    "            .agg(avg(\"AVG_PAYMENT_EFFICIENCY\").alias(\"avg_efficiency\")) \\\n",
    "            .orderBy(\"default payment next month\")\n",
    "        \n",
    "        print(f\"\\n   Average Efficiency by Default Status:\")\n",
    "        for row in efficiency_by_default.collect():\n",
    "            default_status = \"No Default\" if row['default payment next month'] == 0 else \"Default\"\n",
    "            avg_efficiency = row['avg_efficiency']\n",
    "            print(f\"      {default_status}: {avg_efficiency:.3f} ({avg_efficiency*100:.1f}%)\")\n",
    "\n",
    "# Payment consistency analysis\n",
    "if 'PAYMENT_CONSISTENCY_SCORE' in df_features.columns:\n",
    "    print(f\"\\nüìä PAYMENT CONSISTENCY ANALYSIS:\")\n",
    "    \n",
    "    # Analyze consistency score distribution\n",
    "    consistency_percentiles = df_features.select(\n",
    "        expr('percentile_approx(PAYMENT_CONSISTENCY_SCORE, 0.1)').alias('p10'),\n",
    "        expr('percentile_approx(PAYMENT_CONSISTENCY_SCORE, 0.25)').alias('q1'),\n",
    "        expr('percentile_approx(PAYMENT_CONSISTENCY_SCORE, 0.5)').alias('median'),\n",
    "        expr('percentile_approx(PAYMENT_CONSISTENCY_SCORE, 0.75)').alias('q3'),\n",
    "        expr('percentile_approx(PAYMENT_CONSISTENCY_SCORE, 0.9)').alias('p90')\n",
    "    ).collect()[0]\n",
    "    \n",
    "    print(f\"   Consistency Score Percentiles:\")\n",
    "    print(f\"      10th percentile: {consistency_percentiles['p10']:.3f}\")\n",
    "    print(f\"      25th percentile: {consistency_percentiles['q1']:.3f}\")\n",
    "    print(f\"      Median: {consistency_percentiles['median']:.3f}\")\n",
    "    print(f\"      75th percentile: {consistency_percentiles['q3']:.3f}\")\n",
    "    print(f\"      90th percentile: {consistency_percentiles['p90']:.3f}\")\n",
    "    \n",
    "    # High consistency customers\n",
    "    high_consistency_threshold = consistency_percentiles['q3']\n",
    "    high_consistency_count = df_features.filter(\n",
    "        col(\"PAYMENT_CONSISTENCY_SCORE\") >= high_consistency_threshold\n",
    "    ).count()\n",
    "    \n",
    "    print(f\"\\n   High Consistency (‚â•75th percentile):\")\n",
    "    print(f\"      Count: {high_consistency_count:,} ({high_consistency_count/df_features.count()*100:.1f}%)\")\n",
    "    print(f\"      Threshold: {high_consistency_threshold:.3f}\")\n",
    "\n",
    "# Payment efficiency trend analysis\n",
    "if 'PAYMENT_EFFICIENCY_TREND' in df_features.columns:\n",
    "    print(f\"\\nüìà PAYMENT EFFICIENCY TREND ANALYSIS:\")\n",
    "    \n",
    "    # Categorize efficiency trends\n",
    "    trend_categories = df_features.withColumn(\n",
    "        \"EFFICIENCY_TREND_CATEGORY\",\n",
    "        when(col(\"PAYMENT_EFFICIENCY_TREND\") > 0.1, \"Improving Efficiency\")\n",
    "        .when(col(\"PAYMENT_EFFICIENCY_TREND\") > -0.1, \"Stable Efficiency\")\n",
    "        .otherwise(\"Declining Efficiency\")\n",
    "    ).groupBy(\"EFFICIENCY_TREND_CATEGORY\").count().orderBy(desc(\"count\"))\n",
    "    \n",
    "    print(f\"   Efficiency Trend Distribution:\")\n",
    "    for row in trend_categories.collect():\n",
    "        category = row['EFFICIENCY_TREND_CATEGORY']\n",
    "        count = row['count']\n",
    "        percentage = count / df_features.count() * 100\n",
    "        print(f\"      {category}: {count:,} ({percentage:.1f}%)\")\n",
    "\n",
    "print(f\"\\n‚úÖ Phase 4 completed: {len(efficiency_features)} payment efficiency and consistency features created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 5: Credit Utilization and Risk Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create credit utilization features\n",
    "print(\"üí≥ PHASE 5: CREDIT UTILIZATION AND RISK FEATURES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Apply credit utilization feature engineering\n",
    "df_features = feature_engineer.create_credit_utilization_features(df_features)\n",
    "\n",
    "# Analyze credit utilization features\n",
    "utilization_features = [\n",
    "    'CREDIT_UTILIZATION_RATIO', 'CREDIT_BUFFER', 'CREDIT_UTILIZATION_TREND'\n",
    "]\n",
    "\n",
    "print(f\"\\nüìä CREDIT UTILIZATION FEATURES ANALYSIS:\")\n",
    "print(f\"{'Feature':<25} {'Mean':<12} {'Std':<12} {'Min':<12} {'Max':<12}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for feature in utilization_features:\n",
    "    if feature in df_features.columns:\n",
    "        stats = df_features.select(\n",
    "            avg(feature).alias('mean'),\n",
    "            stddev(feature).alias('std'),\n",
    "            min(feature).alias('min'),\n",
    "            max(feature).alias('max')\n",
    "        ).collect()[0]\n",
    "        \n",
    "        if feature == 'CREDIT_BUFFER':\n",
    "            # Display credit buffer in thousands\n",
    "            print(f\"{feature:<25} {stats['mean']/1000:<12.1f}K {stats['std']/1000:<12.1f}K \"\n",
    "                  f\"{stats['min']/1000:<12.1f}K {stats['max']/1000:<12.1f}K\")\n",
    "        else:\n",
    "            print(f\"{feature:<25} {stats['mean']:<12.3f} {stats['std']:<12.3f} \"\n",
    "                  f\"{stats['min']:<12.3f} {stats['max']:<12.3f}\")\n",
    "\n",
    "# Credit utilization ratio analysis\n",
    "if 'CREDIT_UTILIZATION_RATIO' in df_features.columns:\n",
    "    print(f\"\\nüí≥ CREDIT UTILIZATION RATIO ANALYSIS:\")\n",
    "    \n",
    "    # Categorize utilization ratios\n",
    "    utilization_categories = df_features.withColumn(\n",
    "        \"UTILIZATION_CATEGORY\",\n",
    "        when(col(\"CREDIT_UTILIZATION_RATIO\") <= 0.1, \"Very Low (‚â§10%)\")\n",
    "        .when(col(\"CREDIT_UTILIZATION_RATIO\") <= 0.3, \"Low (10-30%)\")\n",
    "        .when(col(\"CREDIT_UTILIZATION_RATIO\") <= 0.5, \"Medium (30-50%)\")\n",
    "        .when(col(\"CREDIT_UTILIZATION_RATIO\") <= 0.8, \"High (50-80%)\")\n",
    "        .when(col(\"CREDIT_UTILIZATION_RATIO\") <= 1.0, \"Very High (80-100%)\")\n",
    "        .otherwise(\"Over Limit (>100%)\")\n",
    "    ).groupBy(\"UTILIZATION_CATEGORY\").count().orderBy(desc(\"count\"))\n",
    "    \n",
    "    print(f\"   Credit Utilization Distribution:\")\n",
    "    for row in utilization_categories.collect():\n",
    "        category = row['UTILIZATION_CATEGORY']\n",
    "        count = row['count']\n",
    "        percentage = count / df_features.count() * 100\n",
    "        print(f\"      {category}: {count:,} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Utilization vs default analysis\n",
    "    if 'default payment next month' in df_features.columns:\n",
    "        utilization_default = df_features.groupBy(\"default payment next month\") \\\n",
    "            .agg(avg(\"CREDIT_UTILIZATION_RATIO\").alias(\"avg_utilization\")) \\\n",
    "            .orderBy(\"default payment next month\")\n",
    "        \n",
    "        print(f\"\\n   Average Utilization by Default Status:\")\n",
    "        for row in utilization_default.collect():\n",
    "            default_status = \"No Default\" if row['default payment next month'] == 0 else \"Default\"\n",
    "            avg_utilization = row['avg_utilization']\n",
    "            print(f\"      {default_status}: {avg_utilization:.3f} ({avg_utilization*100:.1f}%)\")\n",
    "\n",
    "# Credit buffer analysis\n",
    "if 'CREDIT_BUFFER' in df_features.columns:\n",
    "    print(f\"\\nüí∞ CREDIT BUFFER ANALYSIS:\")\n",
    "    \n",
    "    # Analyze credit buffer distribution\n",
    "    buffer_stats = df_features.select(\n",
    "        avg(\"CREDIT_BUFFER\").alias('avg_buffer'),\n",
    "        expr('percentile_approx(CREDIT_BUFFER, 0.25)').alias('q1'),\n",
    "        expr('percentile_approx(CREDIT_BUFFER, 0.5)').alias('median'),\n",
    "        expr('percentile_approx(CREDIT_BUFFER, 0.75)').alias('q3')\n",
    "    ).collect()[0]\n",
    "    \n",
    "    print(f\"   Credit Buffer Statistics (NT$):\")\n",
    "    print(f\"      Average: NT$ {buffer_stats['avg_buffer']:,.0f}\")\n",
    "    print(f\"      25th percentile: NT$ {buffer_stats['q1']:,.0f}\")\n",
    "    print(f\"      Median: NT$ {buffer_stats['median']:,.0f}\")\n",
    "    print(f\"      75th percentile: NT$ {buffer_stats['q3']:,.0f}\")\n",
    "    \n",
    "    # Low buffer customers (potential risk)\n",
    "    low_buffer_count = df_features.filter(col(\"CREDIT_BUFFER\") < 10000).count()\n",
    "    print(f\"\\n   Low Buffer (<NT$ 10,000):\")\n",
    "    print(f\"      Count: {low_buffer_count:,} ({low_buffer_count/df_features.count()*100:.1f}%)\")\n",
    "\n",
    "# Credit utilization trend analysis\n",
    "if 'CREDIT_UTILIZATION_TREND' in df_features.columns:\n",
    "    print(f\"\\nüìà CREDIT UTILIZATION TREND ANALYSIS:\")\n",
    "    \n",
    "    # Categorize utilization trends\n",
    "    trend_categories = df_features.withColumn(\n",
    "        \"UTILIZATION_TREND_CATEGORY\",\n",
    "        when(col(\"CREDIT_UTILIZATION_TREND\") > 0.1, \"Increasing Utilization\")\n",
    "        .when(col(\"CREDIT_UTILIZATION_TREND\") > -0.1, \"Stable Utilization\")\n",
    "        .otherwise(\"Decreasing Utilization\")\n",
    "    ).groupBy(\"UTILIZATION_TREND_CATEGORY\").count().orderBy(desc(\"count\"))\n",
    "    \n",
    "    print(f\"   Utilization Trend Distribution:\")\n",
    "    for row in trend_categories.collect():\n",
    "        category = row['UTILIZATION_TREND_CATEGORY']\n",
    "        count = row['count']\n",
    "        percentage = count / df_features.count() * 100\n",
    "        print(f\"      {category}: {count:,} ({percentage:.1f}%)\")\n",
    "\n",
    "print(f\"\\n‚úÖ Phase 5 completed: {len(utilization_features)} credit utilization features created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 6: Behavioral Classification and Risk Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create behavioral classification features\n",
    "print(\"üéØ PHASE 6: BEHAVIORAL CLASSIFICATION AND RISK SCORING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Apply behavioral classification feature engineering\n",
    "df_features = feature_engineer.create_behavioral_classification_features(df_features)\n",
    "\n",
    "# Apply risk scoring feature engineering\n",
    "df_features = feature_engineer.create_risk_scoring_features(df_features)\n",
    "\n",
    "# Analyze behavioral features\n",
    "behavioral_features = [\n",
    "    'PAYMENT_BEHAVIOR_TYPE', 'TEMPORAL_RISK_LEVEL', 'CUSTOMER_SEGMENT',\n",
    "    'TEMPORAL_RISK_SCORE', 'RISK_SCORE_CATEGORY'\n",
    "]\n",
    "\n",
    "print(f\"\\nüéØ BEHAVIORAL CLASSIFICATION ANALYSIS:\")\n",
    "\n",
    "# Payment behavior type analysis\n",
    "if 'PAYMENT_BEHAVIOR_TYPE' in df_features.columns:\n",
    "    print(f\"\\n   Payment Behavior Type Distribution:\")\n",
    "    behavior_dist = df_features.groupBy(\"PAYMENT_BEHAVIOR_TYPE\").count().orderBy(desc(\"count\"))\n",
    "    \n",
    "    for row in behavior_dist.collect():\n",
    "        behavior_type = row['PAYMENT_BEHAVIOR_TYPE']\n",
    "        count = row['count']\n",
    "        percentage = count / df_features.count() * 100\n",
    "        print(f\"      {behavior_type}: {count:,} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Behavior type vs default analysis\n",
    "    if 'default payment next month' in df_features.columns:\n",
    "        behavior_default = df_features.groupBy(\"PAYMENT_BEHAVIOR_TYPE\", \"default payment next month\") \\\n",
    "            .count() \\\n",
    "            .groupBy(\"PAYMENT_BEHAVIOR_TYPE\") \\\n",
    "            .pivot(\"default payment next month\") \\\n",
    "            .sum(\"count\") \\\n",
    "            .fillna(0)\n",
    "        \n",
    "        behavior_default = behavior_default.withColumn(\n",
    "            \"total\", col(\"0\") + col(\"1\")\n",
    "        ).withColumn(\n",
    "            \"default_rate\", col(\"1\") / col(\"total\") * 100\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n   Default Rate by Payment Behavior Type:\")\n",
    "        print(f\"   {'Behavior Type':<20} {'No Default':<12} {'Default':<10} {'Default Rate':<12}\")\n",
    "        print(\"   \" + \"-\" * 60)\n",
    "        \n",
    "        for row in behavior_default.collect():\n",
    "            behavior_type = row['PAYMENT_BEHAVIOR_TYPE']\n",
    "            no_default = int(row['0']) if row['0'] else 0\n",
    "            default = int(row['1']) if row['1'] else 0\n",
    "            default_rate = row['default_rate']\n",
    "            print(f\"   {behavior_type:<20} {no_default:<12,} {default:<10,} {default_rate:<12.1f}%\")\n",
    "\n",
    "# Temporal risk level analysis\n",
    "if 'TEMPORAL_RISK_LEVEL' in df_features.columns:\n",
    "    print(f\"\\nüìä TEMPORAL RISK LEVEL ANALYSIS:\")\n",
    "    \n",
    "    risk_level_dist = df"
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python", 
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
